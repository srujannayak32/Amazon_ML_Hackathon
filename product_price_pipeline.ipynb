{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e57e5c",
   "metadata": {},
   "source": [
    "# Smart Product Pricing Challenge: ML Pipeline\n",
    "\n",
    "This notebook provides a comprehensive end-to-end machine learning pipeline to predict product prices for the Smart Product Pricing Challenge. The goal is to build a reproducible pipeline that achieves a SMAPE (Symmetric Mean Absolute Percentage Error) below 10%.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Environment Configuration](#setup)\n",
    "2. [Data Loading & Exploration](#data-loading)\n",
    "3. [Text Processing & Feature Engineering](#text-processing)\n",
    "4. [Image Processing & Feature Engineering](#image-processing)\n",
    "5. [Model Training & Cross-Validation](#model-training)\n",
    "6. [Ensemble & Stacking](#ensemble)\n",
    "7. [Prediction & Submission Generation](#prediction)\n",
    "8. [Quick Baseline (If You're in a Hurry)](#quick-baseline)\n",
    "9. [Utility Functions](#utility-functions)\n",
    "\n",
    "**Important Ethical Note:** This pipeline does NOT use external price lookup or web-scraped/external price data. Only the provided dataset and files are used for model training and prediction. Using external price data would violate competition rules and is grounds for disqualification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580b8be",
   "metadata": {},
   "source": [
    "## Setup & Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db011018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import gc\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Try importing torch and set its seed if available\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch is not installed. Will use CPU-only models.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect if running on Kaggle or locally\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    # Kaggle environment\n",
    "    BASE_PATH = '/kaggle/input'\n",
    "    OUTPUT_PATH = '/kaggle/working'\n",
    "    print(\"Running in Kaggle environment\")\n",
    "else:\n",
    "    # Local environment - adjust these paths based on your local setup\n",
    "    BASE_PATH = os.path.join(os.path.dirname(os.getcwd()), 'dataset')\n",
    "    OUTPUT_PATH = os.getcwd()\n",
    "    print(f\"Running in local environment: {os.getcwd()}\")\n",
    "    \n",
    "    # Handle specific local paths if needed\n",
    "    if not os.path.exists(BASE_PATH):\n",
    "        possible_paths = [\n",
    "            './dataset',\n",
    "            '../dataset',\n",
    "            './student_resource/dataset',\n",
    "            '../student_resource/dataset'\n",
    "        ]\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                BASE_PATH = path\n",
    "                print(f\"Found dataset at: {BASE_PATH}\")\n",
    "                break\n",
    "        \n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Add utils.py directory to path if needed\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "try:\n",
    "    from utils import download_images\n",
    "    print(\"Successfully imported utils.py\")\n",
    "except ImportError:\n",
    "    sys.path.append('./src')\n",
    "    sys.path.append('../src')\n",
    "    sys.path.append('./student_resource/src')\n",
    "    sys.path.append('../student_resource/src')\n",
    "    try:\n",
    "        from utils import download_images\n",
    "        print(\"Successfully imported utils.py from alternate path\")\n",
    "    except ImportError:\n",
    "        print(\"Warning: Could not import utils.py - image processing may not be available\")\n",
    "        \n",
    "        # Define a fallback download_images function\n",
    "        def download_image(image_link, savefolder):\n",
    "            if(isinstance(image_link, str)):\n",
    "                filename = Path(image_link).name\n",
    "                image_save_path = os.path.join(savefolder, filename)\n",
    "                if(not os.path.exists(image_save_path)):\n",
    "                    try:\n",
    "                        import urllib.request\n",
    "                        urllib.request.urlretrieve(image_link, image_save_path)    \n",
    "                    except Exception as ex:\n",
    "                        print('Warning: Not able to download - {}\\n{}'.format(image_link, ex))\n",
    "                else:\n",
    "                    return\n",
    "            return\n",
    "\n",
    "        def download_images(image_links, download_folder):\n",
    "            if not os.path.exists(download_folder):\n",
    "                os.makedirs(download_folder)\n",
    "            results = []\n",
    "            download_image_partial = partial(download_image, savefolder=download_folder)\n",
    "            with multiprocessing.Pool(min(100, multiprocessing.cpu_count())) as pool:\n",
    "                for result in tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)):\n",
    "                    results.append(result)\n",
    "                pool.close()\n",
    "                pool.join()\n",
    "            print(f\"Downloaded images to {download_folder}\")\n",
    "\n",
    "# Set and validate file paths\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'train.csv')\n",
    "TEST_PATH = os.path.join(BASE_PATH, 'test.csv')\n",
    "SAMPLE_TEST_PATH = os.path.join(BASE_PATH, 'sample_test.csv')\n",
    "SAMPLE_TEST_OUT_PATH = os.path.join(BASE_PATH, 'sample_test_out.csv')\n",
    "OUTPUT_CSV_PATH = os.path.join(OUTPUT_PATH, 'test_out.csv')\n",
    "METRICS_PATH = os.path.join(OUTPUT_PATH, 'oof_metrics.json')\n",
    "CACHE_DIR = os.path.join(OUTPUT_PATH, 'cache')\n",
    "\n",
    "# Create cache directory for storing model artifacts and intermediate features\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Set environment variables for HuggingFace\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_INFERENCE_ENDPOINT\"] = \"\"  # Empty to avoid using HF inference endpoint\n",
    "\n",
    "# Print environment info\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "print(f\"PyTorch available: {TORCH_AVAILABLE}\")\n",
    "if TORCH_AVAILABLE:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set thread limits\n",
    "# Limit CPU usage to avoid competition on shared resources\n",
    "try:\n",
    "    import os\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "except Exception as e:\n",
    "    print(f\"Error setting thread limits: {e}\")\n",
    "\n",
    "print(f\"Setup complete. Data path: {BASE_PATH}, Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6013e90",
   "metadata": {},
   "source": [
    "## Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "try:\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print(f\"Train data shape: {train.shape}\")\n",
    "    print(f\"Test data shape: {test.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Trying to load sample test data instead...\")\n",
    "    try:\n",
    "        train = pd.read_csv(SAMPLE_TEST_PATH)\n",
    "        sample_test_out = pd.read_csv(SAMPLE_TEST_OUT_PATH)\n",
    "        train = pd.merge(train, sample_test_out, on='sample_id', how='inner')\n",
    "        test = train.copy()  # For demonstration purposes only\n",
    "        print(f\"Sample test data shape: {train.shape}\")\n",
    "    except FileNotFoundError as e2:\n",
    "        print(f\"Error loading sample data: {e2}\")\n",
    "        print(\"Please ensure that the dataset files are in the correct location.\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in train data:\")\n",
    "print(train.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Display a few sample rows from the train data\n",
    "print(\"\\nSample rows from train data:\")\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8e5477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price distribution (for train data only)\n",
    "if 'price' in train.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Original price distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(train['price'], bins=50, kde=True)\n",
    "    plt.title('Original Price Distribution')\n",
    "    plt.xlabel('Price')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    # Log-transformed price distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(np.log1p(train['price']), bins=50, kde=True)\n",
    "    plt.title('Log-transformed Price Distribution')\n",
    "    plt.xlabel('Log(Price + 1)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate statistics for the price\n",
    "    price_stats = train['price'].describe()\n",
    "    print(\"Price statistics:\")\n",
    "    print(price_stats)\n",
    "    \n",
    "    # Identify outliers in price\n",
    "    Q1 = train['price'].quantile(0.25)\n",
    "    Q3 = train['price'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = train[(train['price'] < lower_bound) | (train['price'] > upper_bound)]\n",
    "    print(f\"\\nNumber of price outliers: {len(outliers)} ({len(outliers) / len(train) * 100:.2f}%)\")\n",
    "    \n",
    "    # Calculate and display percentiles\n",
    "    percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999]\n",
    "    price_percentiles = np.percentile(train['price'], [p * 100 for p in percentiles])\n",
    "    \n",
    "    percentile_df = pd.DataFrame({\n",
    "        'Percentile': [f\"{p*100}%\" for p in percentiles],\n",
    "        'Price': price_percentiles\n",
    "    })\n",
    "    print(\"\\nPrice percentiles:\")\n",
    "    print(percentile_df)\n",
    "else:\n",
    "    print(\"Price column not available in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the catalog_content field structure\n",
    "print(\"\\nSample catalog_content from first entry:\")\n",
    "if 'catalog_content' in train.columns:\n",
    "    print(train['catalog_content'].iloc[0][:500] + '...')\n",
    "    \n",
    "    # Extract the average length of catalog content\n",
    "    content_lens = train['catalog_content'].str.len()\n",
    "    print(f\"\\nAverage catalog_content length: {content_lens.mean():.2f} characters\")\n",
    "    print(f\"Min catalog_content length: {content_lens.min()} characters\")\n",
    "    print(f\"Max catalog_content length: {content_lens.max()} characters\")\n",
    "    \n",
    "    # Check for patterns in the catalog_content\n",
    "    content_samples = train['catalog_content'].head(3)\n",
    "    patterns = [\n",
    "        \"Item Name:\", \n",
    "        \"Bullet Point\", \n",
    "        \"Product Description:\", \n",
    "        \"Value:\", \n",
    "        \"Unit:\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nChecking for common patterns in catalog_content:\")\n",
    "    for pattern in patterns:\n",
    "        match_count = sum(content_samples.str.contains(pattern, regex=False))\n",
    "        print(f\"Pattern '{pattern}' appears in {match_count} of 3 samples\")\n",
    "else:\n",
    "    print(\"catalog_content column not available in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8dff0a",
   "metadata": {},
   "source": [
    "## Text Processing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text processing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing URLs, special characters, and converting to lowercase\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_ipq(text):\n",
    "    \"\"\"Extract Item Pack Quantity (IPQ) from text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 1\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Look for specific patterns indicating pack quantity\n",
    "    patterns = [\n",
    "        r'pack of (\\d+)',\n",
    "        r'(\\d+)[-\\s]pack',\n",
    "        r'(\\d+)\\s*pcs',\n",
    "        r'(\\d+)\\s*pieces',\n",
    "        r'(\\d+)\\s*count',\n",
    "        r'(\\d+)\\s*ct',\n",
    "        r'(\\d+)\\s*pk',\n",
    "        r'set of (\\d+)',\n",
    "        r'(\\d+)\\s*set',\n",
    "        r'(\\d+)\\s*qty',\n",
    "        r'quantity:\\s*(\\d+)',\n",
    "        r'qty:\\s*(\\d+)',\n",
    "        r'count:\\s*(\\d+)',\n",
    "        r'value:\\s*(\\d+)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                quantity = int(match.group(1))\n",
    "                return max(1, min(quantity, 100))  # Cap at reasonable values\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Check for 'Value: X' pattern which often indicates quantity\n",
    "    value_match = re.search(r'value:\\s*([\\d\\.]+)', text)\n",
    "    if value_match:\n",
    "        try:\n",
    "            value = float(value_match.group(1))\n",
    "            if value >= 1 and value <= 100:\n",
    "                return int(value)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # Default to 1 if no pattern is found\n",
    "    return 1\n",
    "\n",
    "def extract_brand(text):\n",
    "    \"\"\"Extract brand name from text using heuristics\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Look for common brand patterns\n",
    "    brand_patterns = [\n",
    "        r'brand:\\s*([A-Za-z0-9][A-Za-z0-9\\s&\\-]+)',\n",
    "        r'by\\s+([A-Z][A-Za-z0-9\\s&\\-]+)',\n",
    "        r'from\\s+([A-Z][A-Za-z0-9\\s&\\-]+)',\n",
    "        r'item name:\\s*([A-Z][A-Za-z0-9\\s&\\-]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            brand = match.group(1).strip()\n",
    "            # Limit length and filter out generic terms\n",
    "            if len(brand) > 1 and len(brand) < 30:\n",
    "                return brand\n",
    "    \n",
    "    # Try to extract first word from Item Name if it's uppercase\n",
    "    item_name_match = re.search(r'item name:([^,\\n]+)', text, re.IGNORECASE)\n",
    "    if item_name_match:\n",
    "        item_name = item_name_match.group(1).strip()\n",
    "        first_word = item_name.split()[0] if item_name.split() else \"\"\n",
    "        if first_word and first_word[0].isupper() and len(first_word) > 1:\n",
    "            return first_word\n",
    "    \n",
    "    # Try the first word if it's all caps or first letter is capitalized\n",
    "    words = text.split()\n",
    "    if words and len(words[0]) > 1:\n",
    "        if words[0].isupper() or (words[0][0].isupper() and not words[0].isupper()):\n",
    "            return words[0]\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_title(text):\n",
    "    \"\"\"Extract title from catalog content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find item name pattern\n",
    "    item_name_match = re.search(r'item name:(.*?)(?:bullet point|product description|$)', \n",
    "                               text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if item_name_match:\n",
    "        title = item_name_match.group(1).strip()\n",
    "        return title\n",
    "    \n",
    "    # If no specific pattern, take the first line or first 100 characters\n",
    "    lines = text.split('\\n')\n",
    "    if lines:\n",
    "        return lines[0].strip()\n",
    "    \n",
    "    return text[:100] if len(text) > 100 else text\n",
    "\n",
    "def extract_description(text):\n",
    "    \"\"\"Extract product description from catalog content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find product description pattern\n",
    "    desc_match = re.search(r'product description:(.*?)(?:value:|unit:|$)', \n",
    "                           text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if desc_match:\n",
    "        description = desc_match.group(1).strip()\n",
    "        return description\n",
    "    \n",
    "    # If no specific pattern, take everything after the first line\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) > 1:\n",
    "        return ' '.join(lines[1:]).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_bullet_points(text):\n",
    "    \"\"\"Extract bullet points from catalog content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find bullet point pattern\n",
    "    bullet_points = re.findall(r'bullet point \\d+:(.*?)(?=bullet point \\d+:|product description:|$)', \n",
    "                               text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if bullet_points:\n",
    "        return ' '.join([bp.strip() for bp in bullet_points])\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_basic_features(text):\n",
    "    \"\"\"Extract basic text features like length, word count, etc.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Text length\n",
    "    features['text_len'] = len(text)\n",
    "    \n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    features['num_words'] = len(words)\n",
    "    \n",
    "    # Average word length\n",
    "    if features['num_words'] > 0:\n",
    "        features['avg_word_len'] = sum(len(word) for word in words) / features['num_words']\n",
    "    else:\n",
    "        features['avg_word_len'] = 0\n",
    "    \n",
    "    # Number of digits\n",
    "    features['num_digits'] = sum(c.isdigit() for c in text)\n",
    "    \n",
    "    # Number of uppercase letters\n",
    "    features['num_upper'] = sum(c.isupper() for c in text)\n",
    "    \n",
    "    # Number of lowercase letters\n",
    "    features['num_lower'] = sum(c.islower() for c in text)\n",
    "    \n",
    "    # Ratio of uppercase to all letters\n",
    "    total_letters = features['num_upper'] + features['num_lower']\n",
    "    features['upper_ratio'] = features['num_upper'] / total_letters if total_letters > 0 else 0\n",
    "    \n",
    "    # Number of bullet points\n",
    "    features['num_bullets'] = text.lower().count('bullet point')\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Text processing functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32265686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text processing to train and test data\n",
    "def process_catalog_content(df):\n",
    "    \"\"\"\n",
    "    Process catalog content and extract features\n",
    "    Returns the dataframe with additional columns\n",
    "    \"\"\"\n",
    "    if 'catalog_content' not in df.columns:\n",
    "        print(\"Warning: catalog_content not found in dataframe\")\n",
    "        return df\n",
    "    \n",
    "    print(\"Processing catalog content...\")\n",
    "    \n",
    "    # Create copies of the features to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Extract text components\n",
    "    tqdm.pandas(desc=\"Extracting title\")\n",
    "    df_processed['title'] = df_processed['catalog_content'].progress_apply(extract_title)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Extracting description\")\n",
    "    df_processed['description'] = df_processed['catalog_content'].progress_apply(extract_description)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Extracting bullet points\")\n",
    "    df_processed['bullet_points'] = df_processed['catalog_content'].progress_apply(extract_bullet_points)\n",
    "    \n",
    "    # Clean text fields\n",
    "    tqdm.pandas(desc=\"Cleaning title\")\n",
    "    df_processed['clean_title'] = df_processed['title'].progress_apply(clean_text)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Cleaning description\")\n",
    "    df_processed['clean_description'] = df_processed['description'].progress_apply(clean_text)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Cleaning bullet points\")\n",
    "    df_processed['clean_bullet_points'] = df_processed['bullet_points'].progress_apply(clean_text)\n",
    "    \n",
    "    # Combine all cleaned text for a single text feature\n",
    "    df_processed['all_text'] = (df_processed['clean_title'] + ' ' + \n",
    "                             df_processed['clean_description'] + ' ' + \n",
    "                             df_processed['clean_bullet_points'])\n",
    "    \n",
    "    # Extract IPQ and brand\n",
    "    tqdm.pandas(desc=\"Extracting IPQ\")\n",
    "    df_processed['ipq'] = df_processed['catalog_content'].progress_apply(extract_ipq)\n",
    "    \n",
    "    tqdm.pandas(desc=\"Extracting brand\")\n",
    "    df_processed['brand'] = df_processed['catalog_content'].progress_apply(extract_brand)\n",
    "    \n",
    "    # Extract basic text features\n",
    "    tqdm.pandas(desc=\"Extracting basic features\")\n",
    "    basic_features = df_processed['all_text'].progress_apply(extract_basic_features)\n",
    "    \n",
    "    # Convert dictionary of features to columns\n",
    "    for feature in ['text_len', 'num_words', 'avg_word_len', 'num_digits', \n",
    "                   'num_upper', 'num_lower', 'upper_ratio', 'num_bullets']:\n",
    "        df_processed[feature] = basic_features.apply(lambda x: x.get(feature, 0))\n",
    "    \n",
    "    print(\"Catalog content processing completed\")\n",
    "    return df_processed\n",
    "\n",
    "# Apply processing to train and test data\n",
    "print(\"Processing train data...\")\n",
    "train_processed = process_catalog_content(train)\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_processed = process_catalog_content(test)\n",
    "\n",
    "# Print shape of processed data\n",
    "print(f\"Processed train data shape: {train_processed.shape}\")\n",
    "print(f\"Processed test data shape: {test_processed.shape}\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nSample of processed train data:\")\n",
    "display(train_processed[['title', 'description', 'ipq', 'brand', 'text_len', 'num_words']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99583f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features (brand)\n",
    "def encode_categorical_features(train_df, test_df, categorical_cols=['brand']):\n",
    "    \"\"\"Encode categorical features using label encoding with Unknown handling\"\"\"\n",
    "    encoders = {}\n",
    "    train_df_encoded = train_df.copy()\n",
    "    test_df_encoded = test_df.copy()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            print(f\"Encoding {col}...\")\n",
    "            \n",
    "            # Initialize LabelEncoder\n",
    "            encoder = LabelEncoder()\n",
    "            \n",
    "            # Get all unique values from both train and test\n",
    "            all_values = pd.concat([\n",
    "                train_df[col].fillna('Unknown'),\n",
    "                test_df[col].fillna('Unknown')\n",
    "            ]).unique()\n",
    "            \n",
    "            # Make sure 'Unknown' is in the values\n",
    "            if 'Unknown' not in all_values:\n",
    "                all_values = np.append(all_values, 'Unknown')\n",
    "                \n",
    "            # Fit encoder on all values\n",
    "            encoder.fit(all_values)\n",
    "            \n",
    "            # Transform train and test data\n",
    "            train_df_encoded[f'{col}_encoded'] = encoder.transform(train_df[col].fillna('Unknown'))\n",
    "            test_df_encoded[f'{col}_encoded'] = encoder.transform(test_df[col].fillna('Unknown'))\n",
    "            \n",
    "            # Store encoder for later use\n",
    "            encoders[col] = encoder\n",
    "            \n",
    "            # Calculate value counts for information\n",
    "            val_counts = train_df[col].value_counts()\n",
    "            print(f\"Top 5 most common values for {col}: \")\n",
    "            print(val_counts.head(5))\n",
    "            print(f\"Total unique values: {len(val_counts)}\")\n",
    "    \n",
    "    return train_df_encoded, test_df_encoded, encoders\n",
    "\n",
    "# Apply categorical encoding\n",
    "train_encoded, test_encoded, encoders = encode_categorical_features(\n",
    "    train_processed, test_processed, categorical_cols=['brand']\n",
    ")\n",
    "\n",
    "# Display sample of encoded data\n",
    "print(\"\\nSample of encoded train data:\")\n",
    "display(train_encoded[['brand', 'brand_encoded']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TF-IDF features and SVD reduction\n",
    "def generate_tfidf_svd_features(train_df, test_df, text_col='all_text', \n",
    "                               cache_dir=CACHE_DIR, use_cache=True):\n",
    "    \"\"\"Generate TF-IDF features and apply SVD dimensionality reduction\"\"\"\n",
    "    \n",
    "    tfidf_cache_path = os.path.join(cache_dir, 'tfidf_vectorizer.pkl')\n",
    "    svd_cache_path = os.path.join(cache_dir, 'tfidf_svd.pkl')\n",
    "    train_tfidf_svd_cache_path = os.path.join(cache_dir, 'train_tfidf_svd_features.npz')\n",
    "    test_tfidf_svd_cache_path = os.path.join(cache_dir, 'test_tfidf_svd_features.npz')\n",
    "    \n",
    "    # Check if cached files exist and use_cache is True\n",
    "    if (use_cache and os.path.exists(tfidf_cache_path) and \n",
    "        os.path.exists(svd_cache_path) and \n",
    "        os.path.exists(train_tfidf_svd_cache_path) and \n",
    "        os.path.exists(test_tfidf_svd_cache_path)):\n",
    "        \n",
    "        print(\"Loading TF-IDF and SVD features from cache...\")\n",
    "        vectorizer = joblib.load(tfidf_cache_path)\n",
    "        svd = joblib.load(svd_cache_path)\n",
    "        train_tfidf_svd = scipy.sparse.load_npz(train_tfidf_svd_cache_path)\n",
    "        test_tfidf_svd = scipy.sparse.load_npz(test_tfidf_svd_cache_path)\n",
    "        \n",
    "        print(f\"Loaded cached TF-IDF SVD features with {svd.n_components} dimensions\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        \n",
    "        # Configure TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=40000,  # Limit vocabulary size\n",
    "            min_df=3,            # Minimum document frequency\n",
    "            max_df=0.95,         # Maximum document frequency\n",
    "            ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\w{1,}'  # Match words of at least length 1\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the training data\n",
    "        train_text = train_df[text_col].fillna('').values\n",
    "        test_text = test_df[text_col].fillna('').values\n",
    "        \n",
    "        print(\"Fitting TF-IDF vectorizer on training data...\")\n",
    "        train_tfidf = vectorizer.fit_transform(train_text)\n",
    "        \n",
    "        print(\"Transforming test data with TF-IDF vectorizer...\")\n",
    "        test_tfidf = vectorizer.transform(test_text)\n",
    "        \n",
    "        print(f\"TF-IDF features shape - Train: {train_tfidf.shape}, Test: {test_tfidf.shape}\")\n",
    "        \n",
    "        # Apply SVD for dimensionality reduction\n",
    "        n_components = min(256, min(train_tfidf.shape[0], train_tfidf.shape[1]) - 1)\n",
    "        print(f\"Applying SVD to reduce dimensions to {n_components}...\")\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_SEED)\n",
    "        train_tfidf_svd = svd.fit_transform(train_tfidf)\n",
    "        test_tfidf_svd = svd.transform(test_tfidf)\n",
    "        \n",
    "        print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "        \n",
    "        # Cache the results\n",
    "        print(\"Caching TF-IDF and SVD features...\")\n",
    "        joblib.dump(vectorizer, tfidf_cache_path)\n",
    "        joblib.dump(svd, svd_cache_path)\n",
    "        scipy.sparse.save_npz(train_tfidf_svd_cache_path, scipy.sparse.csr_matrix(train_tfidf_svd))\n",
    "        scipy.sparse.save_npz(test_tfidf_svd_cache_path, scipy.sparse.csr_matrix(test_tfidf_svd))\n",
    "    \n",
    "    # Create feature names\n",
    "    tfidf_svd_feature_names = [f'tfidf_svd_{i}' for i in range(train_tfidf_svd.shape[1])]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    train_tfidf_svd_df = pd.DataFrame(\n",
    "        train_tfidf_svd, \n",
    "        columns=tfidf_svd_feature_names,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    test_tfidf_svd_df = pd.DataFrame(\n",
    "        test_tfidf_svd,\n",
    "        columns=tfidf_svd_feature_names,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    return train_tfidf_svd_df, test_tfidf_svd_df, vectorizer, svd\n",
    "\n",
    "# Generate TF-IDF and SVD features\n",
    "train_tfidf_svd_df, test_tfidf_svd_df, tfidf_vectorizer, tfidf_svd = generate_tfidf_svd_features(\n",
    "    train_encoded, test_encoded, text_col='all_text', use_cache=False\n",
    ")\n",
    "\n",
    "# Display shape of TF-IDF SVD features\n",
    "print(f\"TF-IDF SVD features shape - Train: {train_tfidf_svd_df.shape}, Test: {test_tfidf_svd_df.shape}\")\n",
    "print(\"\\nSample of TF-IDF SVD features:\")\n",
    "display(train_tfidf_svd_df.iloc[:3, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb52d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use sentence-transformers for embeddings (with fallback to TF-IDF)\n",
    "def generate_sentence_transformer_features(train_df, test_df, text_col='all_text', \n",
    "                                         model_name='all-MiniLM-L6-v2', \n",
    "                                         cache_dir=CACHE_DIR, use_cache=True,\n",
    "                                         max_samples=None):\n",
    "    \"\"\"\n",
    "    Generate sentence transformer embeddings for text\n",
    "    Will fall back to TF-IDF if transformers not available\n",
    "    \"\"\"\n",
    "    \n",
    "    train_st_cache_path = os.path.join(cache_dir, f'train_{model_name.replace(\"-\", \"_\")}_features.npz')\n",
    "    test_st_cache_path = os.path.join(cache_dir, f'test_{model_name.replace(\"-\", \"_\")}_features.npz')\n",
    "    \n",
    "    # Check if cached files exist and use_cache is True\n",
    "    if use_cache and os.path.exists(train_st_cache_path) and os.path.exists(test_st_cache_path):\n",
    "        print(f\"Loading {model_name} embeddings from cache...\")\n",
    "        train_embeddings = scipy.sparse.load_npz(train_st_cache_path).toarray()\n",
    "        test_embeddings = scipy.sparse.load_npz(test_st_cache_path).toarray()\n",
    "        \n",
    "        print(f\"Loaded cached embeddings with {train_embeddings.shape[1]} dimensions\")\n",
    "        \n",
    "    else:\n",
    "        # Try to import sentence_transformers\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            \n",
    "            print(f\"Loading {model_name} model...\")\n",
    "            model = SentenceTransformer(model_name)\n",
    "            \n",
    "            # Sample data if requested (to save memory)\n",
    "            if max_samples and len(train_df) > max_samples:\n",
    "                print(f\"Sampling {max_samples} out of {len(train_df)} examples for embedding generation...\")\n",
    "                train_sample_idx = np.random.choice(len(train_df), max_samples, replace=False)\n",
    "                train_text = train_df.iloc[train_sample_idx][text_col].fillna('').values\n",
    "            else:\n",
    "                train_text = train_df[text_col].fillna('').values\n",
    "                \n",
    "            test_text = test_df[text_col].fillna('').values\n",
    "            \n",
    "            # Generate embeddings\n",
    "            print(\"Generating embeddings for training data...\")\n",
    "            train_embeddings = model.encode(train_text, show_progress_bar=True, batch_size=32)\n",
    "            \n",
    "            print(\"Generating embeddings for test data...\")\n",
    "            test_embeddings = model.encode(test_text, show_progress_bar=True, batch_size=32)\n",
    "            \n",
    "            # If we sampled, we need to create embeddings for the rest of the data\n",
    "            if max_samples and len(train_df) > max_samples:\n",
    "                print(\"Processing remaining training samples...\")\n",
    "                remaining_idx = [i for i in range(len(train_df)) if i not in train_sample_idx]\n",
    "                batch_size = 1000\n",
    "                \n",
    "                # Process in batches to save memory\n",
    "                full_train_embeddings = np.zeros((len(train_df), train_embeddings.shape[1]))\n",
    "                full_train_embeddings[train_sample_idx] = train_embeddings\n",
    "                \n",
    "                for i in range(0, len(remaining_idx), batch_size):\n",
    "                    batch_idx = remaining_idx[i:i+batch_size]\n",
    "                    batch_text = train_df.iloc[batch_idx][text_col].fillna('').values\n",
    "                    batch_embeddings = model.encode(batch_text, show_progress_bar=True, batch_size=32)\n",
    "                    full_train_embeddings[batch_idx] = batch_embeddings\n",
    "                    \n",
    "                train_embeddings = full_train_embeddings\n",
    "            \n",
    "            # Cache the embeddings\n",
    "            print(\"Caching sentence transformer embeddings...\")\n",
    "            scipy.sparse.save_npz(train_st_cache_path, scipy.sparse.csr_matrix(train_embeddings))\n",
    "            scipy.sparse.save_npz(test_st_cache_path, scipy.sparse.csr_matrix(test_embeddings))\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Sentence transformers not available. Falling back to TF-IDF.\")\n",
    "            \n",
    "            # Use TF-IDF as fallback\n",
    "            tfidf = TfidfVectorizer(max_features=10000)\n",
    "            train_text = train_df[text_col].fillna('').values\n",
    "            test_text = test_df[text_col].fillna('').values\n",
    "            \n",
    "            train_tfidf = tfidf.fit_transform(train_text)\n",
    "            test_tfidf = tfidf.transform(test_text)\n",
    "            \n",
    "            # Apply SVD to get embeddings\n",
    "            svd = TruncatedSVD(n_components=384, random_state=RANDOM_SEED)\n",
    "            train_embeddings = svd.fit_transform(train_tfidf)\n",
    "            test_embeddings = svd.transform(test_tfidf)\n",
    "            \n",
    "            print(\"Generated TF-IDF + SVD fallback embeddings\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sentence transformer embeddings: {e}\")\n",
    "            print(\"Falling back to TF-IDF.\")\n",
    "            \n",
    "            # Use TF-IDF as fallback\n",
    "            tfidf = TfidfVectorizer(max_features=10000)\n",
    "            train_text = train_df[text_col].fillna('').values\n",
    "            test_text = test_df[text_col].fillna('').values\n",
    "            \n",
    "            train_tfidf = tfidf.fit_transform(train_text)\n",
    "            test_tfidf = tfidf.transform(test_text)\n",
    "            \n",
    "            # Apply SVD to get embeddings\n",
    "            svd = TruncatedSVD(n_components=384, random_state=RANDOM_SEED)\n",
    "            train_embeddings = svd.fit_transform(train_tfidf)\n",
    "            test_embeddings = svd.transform(test_tfidf)\n",
    "            \n",
    "            print(\"Generated TF-IDF + SVD fallback embeddings\")\n",
    "    \n",
    "    # Create feature names\n",
    "    embedding_feature_names = [f'st_emb_{i}' for i in range(train_embeddings.shape[1])]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    train_embedding_df = pd.DataFrame(\n",
    "        train_embeddings, \n",
    "        columns=embedding_feature_names,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    test_embedding_df = pd.DataFrame(\n",
    "        test_embeddings,\n",
    "        columns=embedding_feature_names,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    return train_embedding_df, test_embedding_df\n",
    "\n",
    "# Try to generate sentence transformer embeddings\n",
    "try:\n",
    "    train_st_df, test_st_df = generate_sentence_transformer_features(\n",
    "        train_encoded, test_encoded, text_col='all_text', use_cache=False\n",
    "    )\n",
    "    \n",
    "    # Display shape of sentence transformer embeddings\n",
    "    print(f\"Sentence transformer embeddings shape - Train: {train_st_df.shape}, Test: {test_st_df.shape}\")\n",
    "    print(\"\\nSample of sentence transformer embeddings:\")\n",
    "    display(train_st_df.iloc[:3, :5])\n",
    "    \n",
    "    # Set flag indicating that transformer embeddings are available\n",
    "    TRANSFORMER_EMBEDDINGS_AVAILABLE = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not generate sentence transformer embeddings: {e}\")\n",
    "    print(\"Will use TF-IDF SVD embeddings only.\")\n",
    "    TRANSFORMER_EMBEDDINGS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960891e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features for model training\n",
    "def prepare_features_for_modeling(train_df, test_df, tfidf_svd_df_train, tfidf_svd_df_test, \n",
    "                                st_df_train=None, st_df_test=None, use_transformer=True):\n",
    "    \"\"\"\n",
    "    Combine all features for model training\n",
    "    Returns DataFrames with features for training and testing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with the numerical features\n",
    "    numerical_features = [\n",
    "        'ipq', 'text_len', 'num_words', 'avg_word_len', 'num_digits',\n",
    "        'num_upper', 'num_lower', 'upper_ratio', 'num_bullets'\n",
    "    ]\n",
    "    \n",
    "    # Add encoded categorical features\n",
    "    categorical_features = ['brand_encoded']\n",
    "    \n",
    "    # Combine all tabular features\n",
    "    tabular_features = numerical_features + categorical_features\n",
    "    \n",
    "    # Select only features that exist in both train and test\n",
    "    existing_tabular_features = [f for f in tabular_features \n",
    "                               if f in train_df.columns and f in test_df.columns]\n",
    "    \n",
    "    print(f\"Using {len(existing_tabular_features)} tabular features: {existing_tabular_features}\")\n",
    "    \n",
    "    # Start with tabular features\n",
    "    train_features = train_df[existing_tabular_features].copy()\n",
    "    test_features = test_df[existing_tabular_features].copy()\n",
    "    \n",
    "    # Add TF-IDF SVD features\n",
    "    print(\"Adding TF-IDF SVD features...\")\n",
    "    train_features = pd.concat([train_features, tfidf_svd_df_train], axis=1)\n",
    "    test_features = pd.concat([test_features, tfidf_svd_df_test], axis=1)\n",
    "    \n",
    "    # Add sentence transformer features if available and requested\n",
    "    if use_transformer and st_df_train is not None and st_df_test is not None:\n",
    "        print(\"Adding sentence transformer features...\")\n",
    "        train_features = pd.concat([train_features, st_df_train], axis=1)\n",
    "        test_features = pd.concat([test_features, st_df_test], axis=1)\n",
    "    \n",
    "    print(f\"Final feature shapes - Train: {train_features.shape}, Test: {test_features.shape}\")\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "# Prepare feature sets with and without transformers\n",
    "if TRANSFORMER_EMBEDDINGS_AVAILABLE:\n",
    "    train_features_all, test_features_all = prepare_features_for_modeling(\n",
    "        train_encoded, test_encoded, \n",
    "        tfidf_svd_df_train=train_tfidf_svd_df, \n",
    "        tfidf_svd_df_test=test_tfidf_svd_df,\n",
    "        st_df_train=train_st_df, \n",
    "        st_df_test=test_st_df,\n",
    "        use_transformer=True\n",
    "    )\n",
    "else:\n",
    "    train_features_all, test_features_all = prepare_features_for_modeling(\n",
    "        train_encoded, test_encoded, \n",
    "        tfidf_svd_df_train=train_tfidf_svd_df, \n",
    "        tfidf_svd_df_test=test_tfidf_svd_df,\n",
    "        use_transformer=False\n",
    "    )\n",
    "\n",
    "# Also create a set without transformer features for comparison\n",
    "train_features_base, test_features_base = prepare_features_for_modeling(\n",
    "    train_encoded, test_encoded, \n",
    "    tfidf_svd_df_train=train_tfidf_svd_df, \n",
    "    tfidf_svd_df_test=test_tfidf_svd_df,\n",
    "    use_transformer=False\n",
    ")\n",
    "\n",
    "# Display shapes\n",
    "print(\"\\nFeature set shapes:\")\n",
    "print(f\"All features - Train: {train_features_all.shape}, Test: {test_features_all.shape}\")\n",
    "print(f\"Base features - Train: {train_features_base.shape}, Test: {test_features_base.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f31549",
   "metadata": {},
   "source": [
    "## Image Processing & Feature Engineering (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19712230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image downloading and processing\n",
    "def setup_image_processing(train_df, test_df, image_link_col='image_link',\n",
    "                         download_images_func=download_images, \n",
    "                         max_images=None, timeout=60):\n",
    "    \"\"\"Set up image processing functionality with error handling\"\"\"\n",
    "    \n",
    "    # Check if torch and torchvision are available\n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        from torchvision import models, transforms\n",
    "        \n",
    "        print(\"PyTorch and TorchVision are available for image processing\")\n",
    "        TORCH_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"PyTorch or TorchVision not available. Image features will not be used.\")\n",
    "        return None, None, False\n",
    "    \n",
    "    # Define image download function\n",
    "    def download_product_images(df, output_dir, max_samples=None):\n",
    "        \"\"\"Download product images with retry and timeout\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        if max_samples and len(df) > max_samples:\n",
    "            print(f\"Sampling {max_samples} out of {len(df)} images to download\")\n",
    "            df_sample = df.sample(max_samples, random_state=RANDOM_SEED)\n",
    "        else:\n",
    "            df_sample = df\n",
    "        \n",
    "        # Extract image links and sample IDs\n",
    "        image_links = df_sample[image_link_col].values\n",
    "        sample_ids = df_sample['sample_id'].values\n",
    "        \n",
    "        # Create a mapping of sample ID to image filename\n",
    "        sample_id_to_filename = {}\n",
    "        \n",
    "        try:\n",
    "            print(f\"Downloading {len(image_links)} images to {output_dir}\")\n",
    "            download_images_func(image_links, output_dir)\n",
    "            \n",
    "            # Map sample IDs to downloaded filenames\n",
    "            for i, (sample_id, image_link) in enumerate(zip(sample_ids, image_links)):\n",
    "                if isinstance(image_link, str):\n",
    "                    filename = Path(image_link).name\n",
    "                    image_path = os.path.join(output_dir, filename)\n",
    "                    if os.path.exists(image_path):\n",
    "                        sample_id_to_filename[sample_id] = image_path\n",
    "            \n",
    "            print(f\"Successfully downloaded {len(sample_id_to_filename)} images\")\n",
    "            return sample_id_to_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading images: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    # Define feature extraction function\n",
    "    def extract_image_features(sample_id_to_filename, model_name='efficientnet_b0', cache_dir=CACHE_DIR):\n",
    "        \"\"\"Extract features from images using pretrained model\"\"\"\n",
    "        cache_path = os.path.join(cache_dir, f'image_features_{model_name}.pkl')\n",
    "        \n",
    "        # Check if cache exists\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Loading cached image features from {cache_path}\")\n",
    "            sample_id_to_features = joblib.load(cache_path)\n",
    "            return sample_id_to_features\n",
    "        \n",
    "        try:\n",
    "            # Load pretrained model\n",
    "            if model_name == 'efficientnet_b0':\n",
    "                model = models.efficientnet_b0(pretrained=True)\n",
    "            elif model_name == 'resnet18':\n",
    "                model = models.resnet18(pretrained=True)\n",
    "            else:\n",
    "                print(f\"Unknown model: {model_name}. Using efficientnet_b0 instead.\")\n",
    "                model = models.efficientnet_b0(pretrained=True)\n",
    "            \n",
    "            # Remove classification head\n",
    "            if model_name.startswith('efficientnet'):\n",
    "                model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "            else:  # ResNet\n",
    "                model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "            \n",
    "            # Set model to evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            # Move model to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "            \n",
    "            # Define image transforms\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            \n",
    "            # Extract features for each image\n",
    "            sample_id_to_features = {}\n",
    "            for sample_id, image_path in tqdm(sample_id_to_filename.items(), \n",
    "                                             desc=\"Extracting image features\"):\n",
    "                try:\n",
    "                    # Load and transform image\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image_tensor = transform(image).unsqueeze(0)\n",
    "                    \n",
    "                    # Move image to GPU if available\n",
    "                    if torch.cuda.is_available():\n",
    "                        image_tensor = image_tensor.cuda()\n",
    "                    \n",
    "                    # Extract features\n",
    "                    with torch.no_grad():\n",
    "                        features = model(image_tensor)\n",
    "                    \n",
    "                    # Convert to numpy array\n",
    "                    if torch.cuda.is_available():\n",
    "                        features = features.cpu()\n",
    "                    features = features.squeeze().numpy()\n",
    "                    \n",
    "                    # Store features\n",
    "                    sample_id_to_features[sample_id] = features\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting features for image {image_path}: {e}\")\n",
    "            \n",
    "            print(f\"Extracted features for {len(sample_id_to_features)} images\")\n",
    "            \n",
    "            # Cache the features\n",
    "            joblib.dump(sample_id_to_features, cache_path)\n",
    "            \n",
    "            return sample_id_to_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up image feature extraction: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    # Create function to convert image features to DataFrame\n",
    "    def image_features_to_dataframe(df, sample_id_to_features, prefix='img_'):\n",
    "        \"\"\"Convert image features to DataFrame\"\"\"\n",
    "        # Create empty DataFrame for image features\n",
    "        if not sample_id_to_features:\n",
    "            print(\"No image features available\")\n",
    "            return pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Get feature dimensionality from the first feature\n",
    "        first_feature = next(iter(sample_id_to_features.values()))\n",
    "        n_dims = len(first_feature)\n",
    "        feature_names = [f'{prefix}{i}' for i in range(n_dims)]\n",
    "        \n",
    "        # Initialize DataFrame with zeros\n",
    "        image_features_df = pd.DataFrame(\n",
    "            np.zeros((len(df), n_dims)),\n",
    "            columns=feature_names,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Fill in available features\n",
    "        for i, row in df.iterrows():\n",
    "            sample_id = row['sample_id']\n",
    "            if sample_id in sample_id_to_features:\n",
    "                image_features_df.loc[i, feature_names] = sample_id_to_features[sample_id]\n",
    "        \n",
    "        return image_features_df\n",
    "    \n",
    "    return {\n",
    "        'download_images': download_product_images,\n",
    "        'extract_features': extract_image_features,\n",
    "        'features_to_df': image_features_to_dataframe\n",
    "    }, TORCH_AVAILABLE\n",
    "\n",
    "# Check if image processing is available and set up functions\n",
    "image_processing, torch_available_for_images = setup_image_processing(\n",
    "    train_encoded, test_encoded, image_link_col='image_link'\n",
    ")\n",
    "\n",
    "# Set up image processing if available\n",
    "if image_processing:\n",
    "    print(\"Image processing is available. Will attempt to download and process images.\")\n",
    "    USE_IMAGES = True\n",
    "else:\n",
    "    print(\"Image processing is not available. Will proceed without image features.\")\n",
    "    USE_IMAGES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process images (optional, only if image processing is available)\n",
    "if USE_IMAGES:\n",
    "    # Create directories for images\n",
    "    train_images_dir = os.path.join(OUTPUT_PATH, 'train_images')\n",
    "    test_images_dir = os.path.join(OUTPUT_PATH, 'test_images')\n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(test_images_dir, exist_ok=True)\n",
    "    \n",
    "    # Download a sample of images (to save time)\n",
    "    max_train_images = 5000  # Limit to 5000 training images to save time\n",
    "    max_test_images = 1000   # Limit to 1000 test images\n",
    "    \n",
    "    print(\"Downloading training images...\")\n",
    "    train_sample_id_to_filename = image_processing['download_images'](\n",
    "        train_encoded, train_images_dir, max_samples=max_train_images\n",
    "    )\n",
    "    \n",
    "    print(\"Downloading test images...\")\n",
    "    test_sample_id_to_filename = image_processing['download_images'](\n",
    "        test_encoded, test_images_dir, max_samples=max_test_images\n",
    "    )\n",
    "    \n",
    "    # Extract image features\n",
    "    print(\"Extracting image features...\")\n",
    "    train_sample_id_to_features = image_processing['extract_features'](\n",
    "        train_sample_id_to_filename, model_name='efficientnet_b0'\n",
    "    )\n",
    "    \n",
    "    test_sample_id_to_features = image_processing['extract_features'](\n",
    "        test_sample_id_to_filename, model_name='efficientnet_b0'\n",
    "    )\n",
    "    \n",
    "    # Convert image features to DataFrame\n",
    "    print(\"Converting image features to DataFrame...\")\n",
    "    train_image_features_df = image_processing['features_to_df'](\n",
    "        train_encoded, train_sample_id_to_features\n",
    "    )\n",
    "    \n",
    "    test_image_features_df = image_processing['features_to_df'](\n",
    "        test_encoded, test_sample_id_to_features\n",
    "    )\n",
    "    \n",
    "    # Add image features to the feature set\n",
    "    print(\"Adding image features to feature set...\")\n",
    "    train_features_with_images = pd.concat([train_features_all, train_image_features_df], axis=1)\n",
    "    test_features_with_images = pd.concat([test_features_all, test_image_features_df], axis=1)\n",
    "    \n",
    "    print(f\"Features with images - Train: {train_features_with_images.shape}, Test: {test_features_with_images.shape}\")\n",
    "    \n",
    "    # Image feature count\n",
    "    image_feature_count = train_image_features_df.shape[1]\n",
    "    print(f\"Added {image_feature_count} image features\")\n",
    "else:\n",
    "    print(\"Skipping image processing. Will proceed without image features.\")\n",
    "    train_features_with_images = train_features_all\n",
    "    test_features_with_images = test_features_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0268c58",
   "metadata": {},
   "source": [
    "## Model Training & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e33ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric (SMAPE)\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays if they're not already\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Ensure no zeros or negative values (to avoid division by zero)\n",
    "    y_true = np.maximum(y_true, 0.01)\n",
    "    y_pred = np.maximum(y_pred, 0.01)\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Prepare target variable (log-transformed price)\n",
    "if 'price' in train_encoded.columns:\n",
    "    print(\"Log-transforming price for training...\")\n",
    "    train_encoded['log_price'] = np.log1p(train_encoded['price'])\n",
    "    \n",
    "    # Display target distribution after transformation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(train_encoded['log_price'], bins=50, kde=True)\n",
    "    plt.title('Log-transformed Price Distribution')\n",
    "    plt.xlabel('Log(Price + 1)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Handle outliers in the target variable\n",
    "    # Calculate percentile thresholds for capping\n",
    "    upper_threshold = np.percentile(train_encoded['log_price'], 99.9)\n",
    "    \n",
    "    print(f\"Capping log_price at the 99.9th percentile: {upper_threshold:.4f}\")\n",
    "    train_encoded['log_price_capped'] = np.minimum(train_encoded['log_price'], upper_threshold)\n",
    "    \n",
    "    # Use capped version for training\n",
    "    y = train_encoded['log_price_capped']\n",
    "    \n",
    "    print(f\"Target prepared. Shape: {y.shape}\")\n",
    "else:\n",
    "    print(\"Price column not found in training data. Cannot proceed with model training.\")\n",
    "    y = None\n",
    "\n",
    "# Function to create stratification bins for cross-validation\n",
    "def create_strat_bins(y, n_bins=10):\n",
    "    \"\"\"Create bins for stratified cross-validation\"\"\"\n",
    "    return pd.qcut(y, n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Set up cross-validation\n",
    "def setup_cross_validation(X, y, n_splits=5, n_bins=10, random_state=RANDOM_SEED):\n",
    "    \"\"\"Set up stratified K-fold cross-validation\"\"\"\n",
    "    if y is None:\n",
    "        print(\"No target variable available. Cannot set up cross-validation.\")\n",
    "        return None\n",
    "    \n",
    "    # Create bins for stratification\n",
    "    bins = create_strat_bins(y, n_bins)\n",
    "    \n",
    "    # Set up K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Generate fold indices\n",
    "    fold_indices = []\n",
    "    for train_idx, valid_idx in kf.split(X, bins):\n",
    "        fold_indices.append((train_idx, valid_idx))\n",
    "    \n",
    "    return fold_indices\n",
    "\n",
    "# Set up cross-validation folds\n",
    "cv_folds = setup_cross_validation(train_features_all, y, n_splits=5, n_bins=10)\n",
    "\n",
    "if cv_folds:\n",
    "    print(f\"Cross-validation setup complete with {len(cv_folds)} folds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3112dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with cross-validation\n",
    "def train_model_with_cv(X, y, model_class, model_params, folds, \n",
    "                      feature_type='all', model_name='model', \n",
    "                      cache_dir=CACHE_DIR):\n",
    "    \"\"\"\n",
    "    Train model with cross-validation\n",
    "    Returns trained model, OOF predictions, and test predictions\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        print(\"No target variable available. Cannot train model.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Initialize arrays for OOF predictions\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    fold_scores = []\n",
    "    models = []\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} with {feature_type} features\")\n",
    "    print(f\"Feature shape: {X.shape}\")\n",
    "    \n",
    "    # Loop through folds\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"Fold {fold+1}/{len(folds)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # Initialize and fit model\n",
    "        model = model_class(**model_params)\n",
    "        \n",
    "        # Special handling for LightGBM\n",
    "        if isinstance(model, lgb.LGBMRegressor):\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                eval_metric='rmse',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=100\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        valid_preds = model.predict(X_valid)\n",
    "        \n",
    "        # Store OOF predictions\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "        # Transform predictions back to original scale\n",
    "        valid_preds_original = np.expm1(valid_preds)\n",
    "        y_valid_original = np.expm1(y_valid)\n",
    "        \n",
    "        # Calculate SMAPE\n",
    "        fold_smape = smape(y_valid_original, valid_preds_original)\n",
    "        fold_scores.append(fold_smape)\n",
    "        \n",
    "        print(f\"Fold {fold+1} SMAPE: {fold_smape:.4f}\")\n",
    "        \n",
    "        # Store model\n",
    "        models.append(model)\n",
    "    \n",
    "    # Calculate overall score\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"Mean SMAPE across {len(folds)} folds: {mean_score:.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    model_path = os.path.join(cache_dir, f\"{model_name}_{feature_type}_models.pkl\")\n",
    "    joblib.dump(models, model_path)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'models': models,\n",
    "        'oof_preds': oof_preds,\n",
    "        'fold_scores': fold_scores,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "# Define models to train\n",
    "models_to_train = [\n",
    "    {\n",
    "        'name': 'ridge',\n",
    "        'class': Ridge,\n",
    "        'params': {'alpha': 1.0, 'random_state': RANDOM_SEED}\n",
    "    },\n",
    "    {\n",
    "        'name': 'lightgbm',\n",
    "        'class': lgb.LGBMRegressor,\n",
    "        'params': {\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'subsample': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'n_jobs': -1,\n",
    "            'random_state': RANDOM_SEED\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train models with cross-validation\n",
    "model_results = {}\n",
    "for model_config in models_to_train:\n",
    "    model_name = model_config['name']\n",
    "    model_class = model_config['class']\n",
    "    model_params = model_config['params']\n",
    "    \n",
    "    # Train on base features (TF-IDF only)\n",
    "    base_result = train_model_with_cv(\n",
    "        train_features_base, y, model_class, model_params, cv_folds,\n",
    "        feature_type='base', model_name=model_name\n",
    "    )\n",
    "    model_results[f'{model_name}_base'] = base_result\n",
    "    \n",
    "    # Train on all features (including transformer embeddings if available)\n",
    "    all_result = train_model_with_cv(\n",
    "        train_features_all, y, model_class, model_params, cv_folds,\n",
    "        feature_type='all', model_name=model_name\n",
    "    )\n",
    "    model_results[f'{model_name}_all'] = all_result\n",
    "    \n",
    "    # Train on all features + images if image features are available\n",
    "    if USE_IMAGES:\n",
    "        img_result = train_model_with_cv(\n",
    "            train_features_with_images, y, model_class, model_params, cv_folds,\n",
    "            feature_type='with_images', model_name=model_name\n",
    "        )\n",
    "        model_results[f'{model_name}_with_images'] = img_result\n",
    "\n",
    "# Summarize model results\n",
    "print(\"\\nModel performance summary:\")\n",
    "for model_name, result in model_results.items():\n",
    "    if result:  # Check that result is not None\n",
    "        print(f\"{model_name}: Mean SMAPE = {result['mean_score']:.4f}, Fold SMAPEs = {[f'{score:.4f}' for score in result['fold_scores']]}\")\n",
    "\n",
    "# Identify best model based on mean score\n",
    "best_model_name = min(model_results.keys(), key=lambda k: model_results[k]['mean_score'] if model_results[k] else float('inf'))\n",
    "best_model_result = model_results[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name} with Mean SMAPE = {best_model_result['mean_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8da1c",
   "metadata": {},
   "source": [
    "## Ensemble & Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacking ensemble\n",
    "def create_stacking_ensemble(base_models_results, X_train, X_test, y, folds, \n",
    "                          meta_model=Ridge(alpha=0.5), \n",
    "                          model_name='stacking_ensemble'):\n",
    "    \"\"\"\n",
    "    Create a stacking ensemble from base models\n",
    "    Returns meta-model and predictions\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        print(\"No target variable available. Cannot create ensemble.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"\\nCreating stacking ensemble with {len(base_models_results)} base models\")\n",
    "    \n",
    "    # Extract OOF predictions from base models\n",
    "    oof_preds_dict = {name: result['oof_preds'] for name, result in base_models_results.items() if result}\n",
    "    \n",
    "    # Create a DataFrame of OOF predictions\n",
    "    oof_preds_df = pd.DataFrame(oof_preds_dict)\n",
    "    \n",
    "    # Initialize arrays for meta-model OOF predictions\n",
    "    meta_oof_preds = np.zeros(len(X_train))\n",
    "    fold_scores = []\n",
    "    meta_models = []\n",
    "    \n",
    "    # Loop through folds\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"Training meta-model on fold {fold+1}/{len(folds)}\")\n",
    "        \n",
    "        # Split data\n",
    "        meta_X_train = oof_preds_df.iloc[train_idx]\n",
    "        meta_X_valid = oof_preds_df.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # Train meta-model\n",
    "        meta_model_fold = clone(meta_model)\n",
    "        meta_model_fold.fit(meta_X_train, y_train)\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        valid_preds = meta_model_fold.predict(meta_X_valid)\n",
    "        \n",
    "        # Store OOF predictions\n",
    "        meta_oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "        # Transform predictions back to original scale\n",
    "        valid_preds_original = np.expm1(valid_preds)\n",
    "        y_valid_original = np.expm1(y_valid)\n",
    "        \n",
    "        # Calculate SMAPE\n",
    "        fold_smape = smape(y_valid_original, valid_preds_original)\n",
    "        fold_scores.append(fold_smape)\n",
    "        \n",
    "        print(f\"Meta-model fold {fold+1} SMAPE: {fold_smape:.4f}\")\n",
    "        \n",
    "        # Store model\n",
    "        meta_models.append(meta_model_fold)\n",
    "    \n",
    "    # Calculate overall score\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"Meta-model mean SMAPE across {len(folds)} folds: {mean_score:.4f}\")\n",
    "    \n",
    "    # Generate test predictions using all models\n",
    "    test_preds_dict = {}\n",
    "    for name, result in base_models_results.items():\n",
    "        if not result:\n",
    "            continue\n",
    "            \n",
    "        models = result['models']\n",
    "        test_preds_list = []\n",
    "        \n",
    "        # Get predictions from each fold model\n",
    "        for fold_model in models:\n",
    "            test_preds_fold = fold_model.predict(X_test)\n",
    "            test_preds_list.append(test_preds_fold)\n",
    "        \n",
    "        # Average predictions across folds\n",
    "        test_preds_dict[name] = np.mean(test_preds_list, axis=0)\n",
    "    \n",
    "    # Create DataFrame of test predictions\n",
    "    test_preds_df = pd.DataFrame(test_preds_dict)\n",
    "    \n",
    "    # Make meta-model predictions on test data\n",
    "    meta_test_preds_list = []\n",
    "    for meta_model_fold in meta_models:\n",
    "        meta_test_preds_fold = meta_model_fold.predict(test_preds_df)\n",
    "        meta_test_preds_list.append(meta_test_preds_fold)\n",
    "    \n",
    "    # Average meta-model predictions across folds\n",
    "    meta_test_preds = np.mean(meta_test_preds_list, axis=0)\n",
    "    \n",
    "    # Save meta-model\n",
    "    meta_model_path = os.path.join(CACHE_DIR, f\"{model_name}_meta_models.pkl\")\n",
    "    joblib.dump(meta_models, meta_model_path)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'meta_models': meta_models,\n",
    "        'oof_preds': meta_oof_preds,\n",
    "        'test_preds': meta_test_preds,\n",
    "        'fold_scores': fold_scores,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "# Create simple weighted ensemble\n",
    "def create_weighted_ensemble(base_models_results, weights=None):\n",
    "    \"\"\"\n",
    "    Create a weighted ensemble from base models\n",
    "    Returns weighted predictions\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating weighted ensemble\")\n",
    "    \n",
    "    # Extract OOF predictions and scores from base models\n",
    "    oof_preds_dict = {}\n",
    "    test_preds_dict = {}\n",
    "    scores_dict = {}\n",
    "    \n",
    "    for name, result in base_models_results.items():\n",
    "        if result and 'oof_preds' in result and 'mean_score' in result:\n",
    "            oof_preds_dict[name] = result['oof_preds']\n",
    "            scores_dict[name] = result['mean_score']\n",
    "            \n",
    "            # Extract test predictions if available\n",
    "            if 'test_preds' in result:\n",
    "                test_preds_dict[name] = result['test_preds']\n",
    "    \n",
    "    # If no weights provided, use inverse of scores as weights\n",
    "    if weights is None and scores_dict:\n",
    "        # Convert scores to weights (lower score = higher weight)\n",
    "        weights = {}\n",
    "        for name, score in scores_dict.items():\n",
    "            # Avoid division by zero\n",
    "            if score > 0:\n",
    "                weights[name] = 1 / score\n",
    "            else:\n",
    "                weights[name] = 1.0\n",
    "                \n",
    "        # Normalize weights to sum to 1\n",
    "        total_weight = sum(weights.values())\n",
    "        for name in weights:\n",
    "            weights[name] /= total_weight\n",
    "    else:\n",
    "        # Use equal weights if no scores available or weights provided\n",
    "        weights = {name: 1/len(base_models_results) for name in base_models_results}\n",
    "    \n",
    "    print(\"Ensemble weights:\")\n",
    "    for name, weight in weights.items():\n",
    "        print(f\"  {name}: {weight:.4f}\")\n",
    "    \n",
    "    # Create weighted OOF predictions\n",
    "    oof_preds_weighted = np.zeros(len(next(iter(oof_preds_dict.values()))))\n",
    "    for name, preds in oof_preds_dict.items():\n",
    "        oof_preds_weighted += weights[name] * preds\n",
    "    \n",
    "    # Create weighted test predictions if available\n",
    "    if test_preds_dict:\n",
    "        test_preds_weighted = np.zeros(len(next(iter(test_preds_dict.values()))))\n",
    "        for name, preds in test_preds_dict.items():\n",
    "            if name in weights:\n",
    "                test_preds_weighted += weights[name] * preds\n",
    "    else:\n",
    "        test_preds_weighted = None\n",
    "    \n",
    "    return {\n",
    "        'oof_preds': oof_preds_weighted,\n",
    "        'test_preds': test_preds_weighted,\n",
    "        'weights': weights\n",
    "    }\n",
    "\n",
    "# Prepare base models for ensembling\n",
    "if y is not None:\n",
    "    # Select models to include in ensembles\n",
    "    base_models_for_stacking = {\n",
    "        name: result for name, result in model_results.items()\n",
    "        if result and 'oof_preds' in result\n",
    "    }\n",
    "    \n",
    "    # Filter out models that don't have test predictions yet\n",
    "    # We'll need to make predictions with the base models first\n",
    "    for name, result in base_models_for_stacking.items():\n",
    "        if 'models' in result:\n",
    "            models = result['models']\n",
    "            \n",
    "            # Determine which feature set to use\n",
    "            if 'with_images' in name and USE_IMAGES:\n",
    "                X_test_features = test_features_with_images\n",
    "            elif 'all' in name:\n",
    "                X_test_features = test_features_all\n",
    "            else:\n",
    "                X_test_features = test_features_base\n",
    "            \n",
    "            # Make predictions on test data\n",
    "            test_preds_list = []\n",
    "            for fold_model in models:\n",
    "                test_preds_fold = fold_model.predict(X_test_features)\n",
    "                test_preds_list.append(test_preds_fold)\n",
    "            \n",
    "            # Average predictions across folds\n",
    "            result['test_preds'] = np.mean(test_preds_list, axis=0)\n",
    "            print(f\"Generated test predictions for {name}\")\n",
    "    \n",
    "    # Create stacking ensemble with Ridge as meta-model\n",
    "    stacking_result = create_stacking_ensemble(\n",
    "        base_models_for_stacking,\n",
    "        train_features_base,  # Use base features for the meta-model\n",
    "        test_features_base,\n",
    "        y,\n",
    "        cv_folds,\n",
    "        meta_model=Ridge(alpha=0.5),\n",
    "        model_name='stacking_ensemble'\n",
    "    )\n",
    "    \n",
    "    # Create weighted ensemble\n",
    "    weighted_result = create_weighted_ensemble(base_models_for_stacking)\n",
    "    \n",
    "    # Determine which ensemble performed better on OOF data\n",
    "    if stacking_result and weighted_result:\n",
    "        # Transform OOF predictions back to original scale\n",
    "        stacking_oof_original = np.expm1(stacking_result['oof_preds'])\n",
    "        weighted_oof_original = np.expm1(weighted_result['oof_preds'])\n",
    "        y_original = np.expm1(y)\n",
    "        \n",
    "        # Calculate SMAPE\n",
    "        stacking_smape = smape(y_original, stacking_oof_original)\n",
    "        weighted_smape = smape(y_original, weighted_oof_original)\n",
    "        \n",
    "        print(f\"Stacking ensemble SMAPE: {stacking_smape:.4f}\")\n",
    "        print(f\"Weighted ensemble SMAPE: {weighted_smape:.4f}\")\n",
    "        \n",
    "        # Choose the better ensemble\n",
    "        if stacking_smape < weighted_smape:\n",
    "            print(\"Using stacking ensemble for final predictions\")\n",
    "            final_ensemble = 'stacking'\n",
    "            final_test_preds = stacking_result['test_preds']\n",
    "        else:\n",
    "            print(\"Using weighted ensemble for final predictions\")\n",
    "            final_ensemble = 'weighted'\n",
    "            final_test_preds = weighted_result['test_preds']\n",
    "            \n",
    "        # Save ensemble results\n",
    "        ensemble_results = {\n",
    "            'stacking': {\n",
    "                'oof_preds': stacking_result['oof_preds'],\n",
    "                'test_preds': stacking_result['test_preds'],\n",
    "                'smape': stacking_smape\n",
    "            },\n",
    "            'weighted': {\n",
    "                'oof_preds': weighted_result['oof_preds'],\n",
    "                'test_preds': weighted_result['test_preds'],\n",
    "                'smape': weighted_smape\n",
    "            },\n",
    "            'final_ensemble': final_ensemble\n",
    "        }\n",
    "        \n",
    "        # Save ensemble results to disk\n",
    "        joblib.dump(ensemble_results, os.path.join(CACHE_DIR, 'ensemble_results.pkl'))\n",
    "        \n",
    "    else:\n",
    "        print(\"Could not create ensembles. Using best base model instead.\")\n",
    "        final_test_preds = model_results[best_model_name]['test_preds'] \n",
    "else:\n",
    "    print(\"No target variable available. Cannot create ensembles.\")\n",
    "    final_test_preds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280482c0",
   "metadata": {},
   "source": [
    "## Prediction & Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions and create submission\n",
    "def generate_submission(test_df, test_preds, output_path=OUTPUT_CSV_PATH):\n",
    "    \"\"\"Generate submission file with predictions\"\"\"\n",
    "    if test_preds is None:\n",
    "        print(\"No predictions available. Cannot create submission file.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert log predictions back to original scale\n",
    "    test_preds_original = np.expm1(test_preds)\n",
    "    \n",
    "    # Clip to reasonable range (minimum 0.01)\n",
    "    test_preds_clipped = np.maximum(test_preds_original, 0.01)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': test_preds_clipped\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"Saving submission to {output_path}\")\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Print submission statistics\n",
    "    print(\"\\nSubmission statistics:\")\n",
    "    print(f\"Number of rows: {len(submission)}\")\n",
    "    print(f\"Min price: {submission['price'].min():.4f}\")\n",
    "    print(f\"Max price: {submission['price'].max():.4f}\")\n",
    "    print(f\"Mean price: {submission['price'].mean():.4f}\")\n",
    "    print(f\"Median price: {submission['price'].median():.4f}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Generate and save submission file\n",
    "if final_test_preds is not None:\n",
    "    submission = generate_submission(test_encoded, final_test_preds)\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    if y is not None:\n",
    "        metrics = {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'model_results': {name: {'mean_smape': result['mean_score'], \n",
    "                                   'fold_smapes': result['fold_scores']} \n",
    "                           for name, result in model_results.items() if result},\n",
    "            'ensemble_results': {\n",
    "                'stacking_smape': stacking_smape,\n",
    "                'weighted_smape': weighted_smape,\n",
    "                'final_ensemble': final_ensemble\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to JSON\n",
    "        with open(METRICS_PATH, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved metrics to {METRICS_PATH}\")\n",
    "else:\n",
    "    print(\"No predictions available. Cannot create submission file.\")\n",
    "\n",
    "# Print final performance\n",
    "if y is not None:\n",
    "    # Print OOF performance\n",
    "    print(\"\\n----- Final OOF Performance -----\")\n",
    "    print(f\"Best base model ({best_model_name}): {model_results[best_model_name]['mean_score']:.4f}\")\n",
    "    \n",
    "    # Print ensemble performance if available\n",
    "    if 'stacking_smape' in locals() and 'weighted_smape' in locals():\n",
    "        print(f\"Stacking ensemble: {stacking_smape:.4f}\")\n",
    "        print(f\"Weighted ensemble: {weighted_smape:.4f}\")\n",
    "        \n",
    "    # Print final ensemble choice\n",
    "    if 'final_ensemble' in locals():\n",
    "        print(f\"Final ensemble: {final_ensemble}\")\n",
    "        \n",
    "    # Print per-fold performance of best model\n",
    "    print(\"\\nPer-fold SMAPE of best model:\")\n",
    "    for i, score in enumerate(model_results[best_model_name]['fold_scores']):\n",
    "        print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "    # Print submission file path\n",
    "    print(f\"\\nSubmission file: {OUTPUT_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"No target variable available. Cannot evaluate performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5983c22",
   "metadata": {},
   "source": [
    "## Quick Baseline (If You're in a Hurry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Baseline - TF-IDF + Ridge Regression\n",
    "def run_quick_baseline():\n",
    "    \"\"\"\n",
    "    Run a quick baseline model using TF-IDF + Ridge Regression\n",
    "    This should complete in under 10 minutes\n",
    "    \"\"\"\n",
    "    print(\"Running quick baseline model (TF-IDF + Ridge)\")\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        train = pd.read_csv(TRAIN_PATH)\n",
    "        test = pd.read_csv(TEST_PATH)\n",
    "        print(f\"Train data shape: {train.shape}\")\n",
    "        print(f\"Test data shape: {test.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Trying to load sample test data instead...\")\n",
    "        try:\n",
    "            train = pd.read_csv(SAMPLE_TEST_PATH)\n",
    "            sample_test_out = pd.read_csv(SAMPLE_TEST_OUT_PATH)\n",
    "            train = pd.merge(train, sample_test_out, on='sample_id', how='inner')\n",
    "            test = train.copy()  # For demonstration purposes only\n",
    "            print(f\"Sample test data shape: {train.shape}\")\n",
    "        except FileNotFoundError as e2:\n",
    "            print(f\"Error loading sample data: {e2}\")\n",
    "            print(\"Cannot proceed without data.\")\n",
    "            return\n",
    "    \n",
    "    # Verify price column exists\n",
    "    if 'price' not in train.columns:\n",
    "        print(\"Price column not found in training data.\")\n",
    "        return\n",
    "    \n",
    "    # Basic cleaning of catalog_content\n",
    "    print(\"Cleaning text...\")\n",
    "    train['clean_text'] = train['catalog_content'].fillna('').str.lower()\n",
    "    test['clean_text'] = test['catalog_content'].fillna('').str.lower()\n",
    "    \n",
    "    # Extract basic features\n",
    "    print(\"Extracting basic features...\")\n",
    "    train['text_len'] = train['clean_text'].str.len()\n",
    "    test['text_len'] = test['clean_text'].str.len()\n",
    "    \n",
    "    # Generate TF-IDF features\n",
    "    print(\"Generating TF-IDF features...\")\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=20000,  # Limit features for speed\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "        ngram_range=(1, 2)  # Unigrams and bigrams\n",
    "    )\n",
    "    \n",
    "    train_text = train['clean_text'].fillna('').values\n",
    "    test_text = test['clean_text'].fillna('').values\n",
    "    \n",
    "    train_tfidf = tfidf.fit_transform(train_text)\n",
    "    test_tfidf = tfidf.transform(test_text)\n",
    "    \n",
    "    print(f\"TF-IDF features shape - Train: {train_tfidf.shape}, Test: {test_tfidf.shape}\")\n",
    "    \n",
    "    # Apply SVD for dimensionality reduction\n",
    "    print(\"Applying SVD...\")\n",
    "    n_components = 100  # Smaller for speed\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_SEED)\n",
    "    train_tfidf_svd = svd.fit_transform(train_tfidf)\n",
    "    test_tfidf_svd = svd.transform(test_tfidf)\n",
    "    \n",
    "    print(f\"SVD features shape - Train: {train_tfidf_svd.shape}, Test: {test_tfidf_svd.shape}\")\n",
    "    \n",
    "    # Add text_len feature\n",
    "    train_features = np.hstack([train_tfidf_svd, train['text_len'].values.reshape(-1, 1)])\n",
    "    test_features = np.hstack([test_tfidf_svd, test['text_len'].values.reshape(-1, 1)])\n",
    "    \n",
    "    # Log-transform the target\n",
    "    y = np.log1p(train['price'])\n",
    "    \n",
    "    # Train Ridge model\n",
    "    print(\"Training Ridge model...\")\n",
    "    ridge = Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    "    ridge.fit(train_features, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    test_preds = ridge.predict(test_features)\n",
    "    \n",
    "    # Convert back to original scale\n",
    "    test_preds_original = np.expm1(test_preds)\n",
    "    \n",
    "    # Clip to reasonable range (minimum 0.01)\n",
    "    test_preds_clipped = np.maximum(test_preds_original, 0.01)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test['sample_id'],\n",
    "        'price': test_preds_clipped\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(OUTPUT_PATH, 'quick_baseline_submission.csv')\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Quick baseline completed in {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Print submission statistics\n",
    "    print(\"\\nSubmission statistics:\")\n",
    "    print(f\"Number of rows: {len(submission)}\")\n",
    "    print(f\"Min price: {submission['price'].min():.4f}\")\n",
    "    print(f\"Max price: {submission['price'].max():.4f}\")\n",
    "    print(f\"Mean price: {submission['price'].mean():.4f}\")\n",
    "    print(f\"Median price: {submission['price'].median():.4f}\")\n",
    "    \n",
    "    print(f\"\\nSubmission file: {output_path}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Uncomment the line below to run the quick baseline\n",
    "# quick_baseline_submission = run_quick_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cafa86",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions that can be reused across the notebook\n",
    "\n",
    "# Function to generate a command-line runnable script\n",
    "def generate_train_predict_script(output_path=None):\n",
    "    \"\"\"Generate a command-line runnable script for training and prediction\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(OUTPUT_PATH, 'train_predict.py')\n",
    "    \n",
    "    script_content = '''#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Train and predict script for Smart Product Pricing Challenge\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Optional torch imports\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "\n",
    "def setup_paths(base_path=None, output_path=None, dataset_path=None):\n",
    "    \"\"\"Set up paths for data and output\"\"\"\n",
    "    if base_path is None:\n",
    "        # Auto-detect Kaggle environment\n",
    "        if os.path.exists('/kaggle/input'):\n",
    "            base_path = '/kaggle/input'\n",
    "        else:\n",
    "            base_path = '.'\n",
    "    \n",
    "    if output_path is None:\n",
    "        # Auto-detect Kaggle environment\n",
    "        if os.path.exists('/kaggle/working'):\n",
    "            output_path = '/kaggle/working'\n",
    "        else:\n",
    "            output_path = '.'\n",
    "    \n",
    "    if dataset_path is None:\n",
    "        # Try to find dataset path\n",
    "        if os.path.exists(os.path.join(base_path, 'train.csv')):\n",
    "            dataset_path = base_path\n",
    "        else:\n",
    "            # Look in common locations\n",
    "            possible_paths = [\n",
    "                os.path.join(base_path, 'dataset'),\n",
    "                os.path.join(base_path, 'data'),\n",
    "                os.path.join(base_path, 'smart-product-pricing-challenge'),\n",
    "                os.path.join(base_path, 'student_resource', 'dataset')\n",
    "            ]\n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(os.path.join(path, 'train.csv')):\n",
    "                    dataset_path = path\n",
    "                    break\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Create cache directory\n",
    "    cache_dir = os.path.join(output_path, 'cache')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        'base_path': base_path,\n",
    "        'output_path': output_path,\n",
    "        'dataset_path': dataset_path,\n",
    "        'cache_dir': cache_dir,\n",
    "        'train_path': os.path.join(dataset_path, 'train.csv') if dataset_path else None,\n",
    "        'test_path': os.path.join(dataset_path, 'test.csv') if dataset_path else None,\n",
    "        'output_csv_path': os.path.join(output_path, 'test_out.csv'),\n",
    "        'metrics_path': os.path.join(output_path, 'oof_metrics.json')\n",
    "    }\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing URLs, special characters, and converting to lowercase\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove excessive punctuation\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_ipq(text):\n",
    "    \"\"\"Extract Item Pack Quantity (IPQ) from text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 1\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Look for specific patterns indicating pack quantity\n",
    "    patterns = [\n",
    "        r'pack of (\\\\d+)',\n",
    "        r'(\\\\d+)[-\\\\s]pack',\n",
    "        r'(\\\\d+)\\\\s*pcs',\n",
    "        r'(\\\\d+)\\\\s*pieces',\n",
    "        r'(\\\\d+)\\\\s*count',\n",
    "        r'(\\\\d+)\\\\s*ct',\n",
    "        r'(\\\\d+)\\\\s*pk',\n",
    "        r'set of (\\\\d+)',\n",
    "        r'(\\\\d+)\\\\s*set',\n",
    "        r'(\\\\d+)\\\\s*qty',\n",
    "        r'quantity:\\\\s*(\\\\d+)',\n",
    "        r'qty:\\\\s*(\\\\d+)',\n",
    "        r'count:\\\\s*(\\\\d+)',\n",
    "        r'value:\\\\s*(\\\\d+)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            try:\n",
    "                quantity = int(match.group(1))\n",
    "                return max(1, min(quantity, 100))  # Cap at reasonable values\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Check for 'Value: X' pattern which often indicates quantity\n",
    "    value_match = re.search(r'value:\\\\s*([\\\\d\\\\.]+)', text)\n",
    "    if value_match:\n",
    "        try:\n",
    "            value = float(value_match.group(1))\n",
    "            if value >= 1 and value <= 100:\n",
    "                return int(value)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # Default to 1 if no pattern is found\n",
    "    return 1\n",
    "\n",
    "\n",
    "def extract_brand(text):\n",
    "    \"\"\"Extract brand name from text using heuristics\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Look for common brand patterns\n",
    "    brand_patterns = [\n",
    "        r'brand:\\\\s*([A-Za-z0-9][A-Za-z0-9\\\\s&\\\\-]+)',\n",
    "        r'by\\\\s+([A-Z][A-Za-z0-9\\\\s&\\\\-]+)',\n",
    "        r'from\\\\s+([A-Z][A-Za-z0-9\\\\s&\\\\-]+)',\n",
    "        r'item name:\\\\s*([A-Z][A-Za-z0-9\\\\s&\\\\-]+)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in brand_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            brand = match.group(1).strip()\n",
    "            # Limit length and filter out generic terms\n",
    "            if len(brand) > 1 and len(brand) < 30:\n",
    "                return brand\n",
    "    \n",
    "    # Try to extract first word from Item Name if it's uppercase\n",
    "    item_name_match = re.search(r'item name:([^,\\\\n]+)', text, re.IGNORECASE)\n",
    "    if item_name_match:\n",
    "        item_name = item_name_match.group(1).strip()\n",
    "        first_word = item_name.split()[0] if item_name.split() else \"\"\n",
    "        if first_word and first_word[0].isupper() and len(first_word) > 1:\n",
    "            return first_word\n",
    "    \n",
    "    # Try the first word if it's all caps or first letter is capitalized\n",
    "    words = text.split()\n",
    "    if words and len(words[0]) > 1:\n",
    "        if words[0].isupper() or (words[0][0].isupper() and not words[0].isupper()):\n",
    "            return words[0]\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_title(text):\n",
    "    \"\"\"Extract title from catalog content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find item name pattern\n",
    "    item_name_match = re.search(r'item name:(.*?)(?:bullet point|product description|$)', \n",
    "                               text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if item_name_match:\n",
    "        title = item_name_match.group(1).strip()\n",
    "        return title\n",
    "    \n",
    "    # If no specific pattern, take the first line or first 100 characters\n",
    "    lines = text.split('\\\\n')\n",
    "    if lines:\n",
    "        return lines[0].strip()\n",
    "    \n",
    "    return text[:100] if len(text) > 100 else text\n",
    "\n",
    "\n",
    "def extract_description(text):\n",
    "    \"\"\"Extract product description from catalog content\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Try to find product description pattern\n",
    "    desc_match = re.search(r'product description:(.*?)(?:value:|unit:|$)', \n",
    "                           text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if desc_match:\n",
    "        description = desc_match.group(1).strip()\n",
    "        return description\n",
    "    \n",
    "    # If no specific pattern, take everything after the first line\n",
    "    lines = text.split('\\\\n')\n",
    "    if len(lines) > 1:\n",
    "        return ' '.join(lines[1:]).strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_basic_features(text):\n",
    "    \"\"\"Extract basic text features like length, word count, etc.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Text length\n",
    "    features['text_len'] = len(text)\n",
    "    \n",
    "    # Number of words\n",
    "    words = text.split()\n",
    "    features['num_words'] = len(words)\n",
    "    \n",
    "    # Average word length\n",
    "    if features['num_words'] > 0:\n",
    "        features['avg_word_len'] = sum(len(word) for word in words) / features['num_words']\n",
    "    else:\n",
    "        features['avg_word_len'] = 0\n",
    "    \n",
    "    # Number of digits\n",
    "    features['num_digits'] = sum(c.isdigit() for c in text)\n",
    "    \n",
    "    # Number of uppercase letters\n",
    "    features['num_upper'] = sum(c.isupper() for c in text)\n",
    "    \n",
    "    # Number of lowercase letters\n",
    "    features['num_lower'] = sum(c.islower() for c in text)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def process_catalog_content(df):\n",
    "    \"\"\"Process catalog content and extract features\"\"\"\n",
    "    if 'catalog_content' not in df.columns:\n",
    "        print(\"Warning: catalog_content not found in dataframe\")\n",
    "        return df\n",
    "    \n",
    "    print(\"Processing catalog content...\")\n",
    "    \n",
    "    # Create copies of the features to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Extract text components\n",
    "    df_processed['title'] = df_processed['catalog_content'].apply(extract_title)\n",
    "    df_processed['description'] = df_processed['catalog_content'].apply(extract_description)\n",
    "    \n",
    "    # Clean text fields\n",
    "    df_processed['clean_title'] = df_processed['title'].apply(clean_text)\n",
    "    df_processed['clean_description'] = df_processed['description'].apply(clean_text)\n",
    "    \n",
    "    # Combine all cleaned text for a single text feature\n",
    "    df_processed['all_text'] = df_processed['clean_title'] + ' ' + df_processed['clean_description']\n",
    "    \n",
    "    # Extract IPQ and brand\n",
    "    df_processed['ipq'] = df_processed['catalog_content'].apply(extract_ipq)\n",
    "    df_processed['brand'] = df_processed['catalog_content'].apply(extract_brand)\n",
    "    \n",
    "    # Extract basic text features\n",
    "    basic_features = df_processed['all_text'].apply(extract_basic_features)\n",
    "    \n",
    "    # Convert dictionary of features to columns\n",
    "    for feature in ['text_len', 'num_words', 'avg_word_len', 'num_digits', \n",
    "                   'num_upper', 'num_lower']:\n",
    "        df_processed[feature] = basic_features.apply(lambda x: x.get(feature, 0))\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def encode_categorical_features(train_df, test_df, categorical_cols=['brand']):\n",
    "    \"\"\"Encode categorical features using label encoding with Unknown handling\"\"\"\n",
    "    encoders = {}\n",
    "    train_df_encoded = train_df.copy()\n",
    "    test_df_encoded = test_df.copy()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            print(f\"Encoding {col}...\")\n",
    "            \n",
    "            # Initialize LabelEncoder\n",
    "            encoder = LabelEncoder()\n",
    "            \n",
    "            # Get all unique values from both train and test\n",
    "            all_values = pd.concat([\n",
    "                train_df[col].fillna('Unknown'),\n",
    "                test_df[col].fillna('Unknown')\n",
    "            ]).unique()\n",
    "            \n",
    "            # Make sure 'Unknown' is in the values\n",
    "            if 'Unknown' not in all_values:\n",
    "                all_values = np.append(all_values, 'Unknown')\n",
    "                \n",
    "            # Fit encoder on all values\n",
    "            encoder.fit(all_values)\n",
    "            \n",
    "            # Transform train and test data\n",
    "            train_df_encoded[f'{col}_encoded'] = encoder.transform(train_df[col].fillna('Unknown'))\n",
    "            test_df_encoded[f'{col}_encoded'] = encoder.transform(test_df[col].fillna('Unknown'))\n",
    "            \n",
    "            # Store encoder for later use\n",
    "            encoders[col] = encoder\n",
    "    \n",
    "    return train_df_encoded, test_df_encoded, encoders\n",
    "\n",
    "\n",
    "def generate_tfidf_svd_features(train_df, test_df, text_col='all_text', cache_dir=None):\n",
    "    \"\"\"Generate TF-IDF features and apply SVD dimensionality reduction\"\"\"\n",
    "    \n",
    "    if cache_dir:\n",
    "        tfidf_cache_path = os.path.join(cache_dir, 'tfidf_vectorizer.pkl')\n",
    "        svd_cache_path = os.path.join(cache_dir, 'tfidf_svd.pkl')\n",
    "        \n",
    "        # Check if cached files exist\n",
    "        if os.path.exists(tfidf_cache_path) and os.path.exists(svd_cache_path):\n",
    "            print(\"Loading TF-IDF and SVD models from cache...\")\n",
    "            vectorizer = joblib.load(tfidf_cache_path)\n",
    "            svd = joblib.load(svd_cache_path)\n",
    "        else:\n",
    "            vectorizer = None\n",
    "            svd = None\n",
    "    else:\n",
    "        vectorizer = None\n",
    "        svd = None\n",
    "    \n",
    "    if vectorizer is None:\n",
    "        print(\"Generating TF-IDF features...\")\n",
    "        \n",
    "        # Configure TF-IDF vectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=40000,  # Limit vocabulary size\n",
    "            min_df=3,            # Minimum document frequency\n",
    "            max_df=0.95,         # Maximum document frequency\n",
    "            ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "            lowercase=True,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\\\w{1,}'  # Match words of at least length 1\n",
    "        )\n",
    "        \n",
    "        # Fit on training data\n",
    "        train_text = train_df[text_col].fillna('').values\n",
    "        vectorizer.fit(train_text)\n",
    "        \n",
    "        # Cache the vectorizer\n",
    "        if cache_dir:\n",
    "            joblib.dump(vectorizer, tfidf_cache_path)\n",
    "    \n",
    "    # Transform train and test data\n",
    "    train_text = train_df[text_col].fillna('').values\n",
    "    test_text = test_df[text_col].fillna('').values\n",
    "    \n",
    "    print(\"Transforming text data with TF-IDF...\")\n",
    "    train_tfidf = vectorizer.transform(train_text)\n",
    "    test_tfidf = vectorizer.transform(test_text)\n",
    "    \n",
    "    print(f\"TF-IDF features shape - Train: {train_tfidf.shape}, Test: {test_tfidf.shape}\")\n",
    "    \n",
    "    # Apply SVD for dimensionality reduction\n",
    "    if svd is None:\n",
    "        n_components = min(256, min(train_tfidf.shape[0], train_tfidf.shape[1]) - 1)\n",
    "        print(f\"Applying SVD to reduce dimensions to {n_components}...\")\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_SEED)\n",
    "        svd.fit(train_tfidf)\n",
    "        \n",
    "        # Cache the SVD model\n",
    "        if cache_dir:\n",
    "            joblib.dump(svd, svd_cache_path)\n",
    "    \n",
    "    # Transform TF-IDF with SVD\n",
    "    train_tfidf_svd = svd.transform(train_tfidf)\n",
    "    test_tfidf_svd = svd.transform(test_tfidf)\n",
    "    \n",
    "    print(f\"SVD features shape - Train: {train_tfidf_svd.shape}, Test: {test_tfidf_svd.shape}\")\n",
    "    \n",
    "    # Create feature names\n",
    "    tfidf_svd_feature_names = [f'tfidf_svd_{i}' for i in range(train_tfidf_svd.shape[1])]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    train_tfidf_svd_df = pd.DataFrame(\n",
    "        train_tfidf_svd, \n",
    "        columns=tfidf_svd_feature_names,\n",
    "        index=train_df.index\n",
    "    )\n",
    "    \n",
    "    test_tfidf_svd_df = pd.DataFrame(\n",
    "        test_tfidf_svd,\n",
    "        columns=tfidf_svd_feature_names,\n",
    "        index=test_df.index\n",
    "    )\n",
    "    \n",
    "    return train_tfidf_svd_df, test_tfidf_svd_df, vectorizer, svd\n",
    "\n",
    "\n",
    "def prepare_features_for_modeling(train_df, test_df, tfidf_svd_df_train, tfidf_svd_df_test):\n",
    "    \"\"\"Combine all features for model training\"\"\"\n",
    "    \n",
    "    # Start with the numerical features\n",
    "    numerical_features = [\n",
    "        'ipq', 'text_len', 'num_words', 'avg_word_len', 'num_digits',\n",
    "        'num_upper', 'num_lower'\n",
    "    ]\n",
    "    \n",
    "    # Add encoded categorical features\n",
    "    categorical_features = ['brand_encoded']\n",
    "    \n",
    "    # Combine all tabular features\n",
    "    tabular_features = numerical_features + categorical_features\n",
    "    \n",
    "    # Select only features that exist in both train and test\n",
    "    existing_tabular_features = [f for f in tabular_features \n",
    "                               if f in train_df.columns and f in test_df.columns]\n",
    "    \n",
    "    print(f\"Using {len(existing_tabular_features)} tabular features\")\n",
    "    \n",
    "    # Start with tabular features\n",
    "    train_features = train_df[existing_tabular_features].copy()\n",
    "    test_features = test_df[existing_tabular_features].copy()\n",
    "    \n",
    "    # Add TF-IDF SVD features\n",
    "    print(\"Adding TF-IDF SVD features...\")\n",
    "    train_features = pd.concat([train_features, tfidf_svd_df_train], axis=1)\n",
    "    test_features = pd.concat([test_features, tfidf_svd_df_test], axis=1)\n",
    "    \n",
    "    print(f\"Final feature shapes - Train: {train_features.shape}, Test: {test_features.shape}\")\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculate Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Ensure no zeros (to avoid division by zero)\n",
    "    y_true = np.maximum(y_true, 0.01)\n",
    "    y_pred = np.maximum(y_pred, 0.01)\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "\n",
    "def create_strat_bins(y, n_bins=10):\n",
    "    \"\"\"Create bins for stratified cross-validation\"\"\"\n",
    "    return pd.qcut(y, n_bins, labels=False, duplicates='drop')\n",
    "\n",
    "\n",
    "def setup_cross_validation(X, y, n_splits=5, n_bins=10, random_state=RANDOM_SEED):\n",
    "    \"\"\"Set up stratified K-fold cross-validation\"\"\"\n",
    "    # Create bins for stratification\n",
    "    bins = create_strat_bins(y, n_bins)\n",
    "    \n",
    "    # Set up K-fold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Generate fold indices\n",
    "    fold_indices = []\n",
    "    for train_idx, valid_idx in kf.split(X, bins):\n",
    "        fold_indices.append((train_idx, valid_idx))\n",
    "    \n",
    "    return fold_indices\n",
    "\n",
    "\n",
    "def train_model_with_cv(X, y, model_class, model_params, folds, model_name='model', cache_dir=None):\n",
    "    \"\"\"Train model with cross-validation\"\"\"\n",
    "    # Initialize arrays for OOF predictions\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    fold_scores = []\n",
    "    models = []\n",
    "    \n",
    "    print(f\"\\\\nTraining {model_name}\")\n",
    "    \n",
    "    # Loop through folds\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"Fold {fold+1}/{len(folds)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        # Initialize and fit model\n",
    "        model = model_class(**model_params)\n",
    "        \n",
    "        # Special handling for LightGBM\n",
    "        if isinstance(model, lgb.LGBMRegressor):\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                eval_metric='rmse',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=100\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        valid_preds = model.predict(X_valid)\n",
    "        \n",
    "        # Store OOF predictions\n",
    "        oof_preds[valid_idx] = valid_preds\n",
    "        \n",
    "        # Transform predictions back to original scale\n",
    "        valid_preds_original = np.expm1(valid_preds)\n",
    "        y_valid_original = np.expm1(y_valid)\n",
    "        \n",
    "        # Calculate SMAPE\n",
    "        fold_smape = smape(y_valid_original, valid_preds_original)\n",
    "        fold_scores.append(fold_smape)\n",
    "        \n",
    "        print(f\"Fold {fold+1} SMAPE: {fold_smape:.4f}\")\n",
    "        \n",
    "        # Store model\n",
    "        models.append(model)\n",
    "    \n",
    "    # Calculate overall score\n",
    "    mean_score = np.mean(fold_scores)\n",
    "    print(f\"Mean SMAPE across {len(folds)} folds: {mean_score:.4f}\")\n",
    "    \n",
    "    # Save models\n",
    "    if cache_dir:\n",
    "        model_path = os.path.join(cache_dir, f\"{model_name}_models.pkl\")\n",
    "        joblib.dump(models, model_path)\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'oof_preds': oof_preds,\n",
    "        'fold_scores': fold_scores,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_predictions(models, X_test):\n",
    "    \"\"\"Generate predictions using multiple fold models\"\"\"\n",
    "    # Get predictions from each fold model\n",
    "    test_preds_list = []\n",
    "    for model in models:\n",
    "        test_preds_fold = model.predict(X_test)\n",
    "        test_preds_list.append(test_preds_fold)\n",
    "    \n",
    "    # Average predictions across folds\n",
    "    test_preds = np.mean(test_preds_list, axis=0)\n",
    "    \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "def generate_submission(test_df, test_preds, output_path):\n",
    "    \"\"\"Generate submission file with predictions\"\"\"\n",
    "    # Convert log predictions back to original scale\n",
    "    test_preds_original = np.expm1(test_preds)\n",
    "    \n",
    "    # Clip to reasonable range (minimum 0.01)\n",
    "    test_preds_clipped = np.maximum(test_preds_original, 0.01)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'sample_id': test_df['sample_id'],\n",
    "        'price': test_preds_clipped\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"Saving submission to {output_path}\")\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function for training and prediction\"\"\"\n",
    "    print(\"Starting Smart Product Pricing pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set up paths\n",
    "    paths = setup_paths(args.base_path, args.output_path, args.dataset_path)\n",
    "    print(f\"Using dataset path: {paths['dataset_path']}\")\n",
    "    print(f\"Output path: {paths['output_path']}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\\\nLoading data...\")\n",
    "    try:\n",
    "        train = pd.read_csv(paths['train_path'])\n",
    "        test = pd.read_csv(paths['test_path'])\n",
    "        print(f\"Train data shape: {train.shape}\")\n",
    "        print(f\"Test data shape: {test.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure the dataset files are in the correct location.\")\n",
    "        return 1\n",
    "    \n",
    "    # Process catalog content\n",
    "    print(\"\\\\nProcessing catalog content...\")\n",
    "    train_processed = process_catalog_content(train)\n",
    "    test_processed = process_catalog_content(test)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    print(\"\\\\nEncoding categorical features...\")\n",
    "    train_encoded, test_encoded, encoders = encode_categorical_features(\n",
    "        train_processed, test_processed, categorical_cols=['brand']\n",
    "    )\n",
    "    \n",
    "    # Generate TF-IDF SVD features\n",
    "    print(\"\\\\nGenerating TF-IDF and SVD features...\")\n",
    "    train_tfidf_svd_df, test_tfidf_svd_df, tfidf_vectorizer, tfidf_svd = generate_tfidf_svd_features(\n",
    "        train_encoded, test_encoded, text_col='all_text', cache_dir=paths['cache_dir']\n",
    "    )\n",
    "    \n",
    "    # Prepare features for modeling\n",
    "    print(\"\\\\nPreparing features for modeling...\")\n",
    "    train_features, test_features = prepare_features_for_modeling(\n",
    "        train_encoded, test_encoded, \n",
    "        tfidf_svd_df_train=train_tfidf_svd_df, \n",
    "        tfidf_svd_df_test=test_tfidf_svd_df\n",
    "    )\n",
    "    \n",
    "    # Prepare target variable (log-transformed price)\n",
    "    print(\"\\\\nPreparing target variable...\")\n",
    "    train_encoded['log_price'] = np.log1p(train_encoded['price'])\n",
    "    \n",
    "    # Handle outliers in the target variable\n",
    "    upper_threshold = np.percentile(train_encoded['log_price'], 99.9)\n",
    "    train_encoded['log_price_capped'] = np.minimum(train_encoded['log_price'], upper_threshold)\n",
    "    \n",
    "    # Use capped version for training\n",
    "    y = train_encoded['log_price_capped']\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    print(\"\\\\nSetting up cross-validation...\")\n",
    "    cv_folds = setup_cross_validation(train_features, y, n_splits=5, n_bins=10)\n",
    "    \n",
    "    # Define models to train\n",
    "    if args.model == 'lightgbm':\n",
    "        print(\"\\\\nTraining LightGBM model...\")\n",
    "        model_result = train_model_with_cv(\n",
    "            train_features, y, \n",
    "            lgb.LGBMRegressor,\n",
    "            {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': 0.05,\n",
    "                'num_leaves': 31,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'subsample': 0.8,\n",
    "                'reg_alpha': 0.1,\n",
    "                'reg_lambda': 0.1,\n",
    "                'n_jobs': -1,\n",
    "                'random_state': RANDOM_SEED\n",
    "            },\n",
    "            cv_folds,\n",
    "            model_name='lightgbm',\n",
    "            cache_dir=paths['cache_dir']\n",
    "        )\n",
    "    elif args.model == 'ridge':\n",
    "        print(\"\\\\nTraining Ridge model...\")\n",
    "        model_result = train_model_with_cv(\n",
    "            train_features, y,\n",
    "            Ridge,\n",
    "            {'alpha': 1.0, 'random_state': RANDOM_SEED},\n",
    "            cv_folds,\n",
    "            model_name='ridge',\n",
    "            cache_dir=paths['cache_dir']\n",
    "        )\n",
    "    else:\n",
    "        # Train both models\n",
    "        print(\"\\\\nTraining Ridge model...\")\n",
    "        ridge_result = train_model_with_cv(\n",
    "            train_features, y,\n",
    "            Ridge,\n",
    "            {'alpha': 1.0, 'random_state': RANDOM_SEED},\n",
    "            cv_folds,\n",
    "            model_name='ridge',\n",
    "            cache_dir=paths['cache_dir']\n",
    "        )\n",
    "        \n",
    "        print(\"\\\\nTraining LightGBM model...\")\n",
    "        lgb_result = train_model_with_cv(\n",
    "            train_features, y, \n",
    "            lgb.LGBMRegressor,\n",
    "            {\n",
    "                'n_estimators': 1000,\n",
    "                'learning_rate': 0.05,\n",
    "                'num_leaves': 31,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'subsample': 0.8,\n",
    "                'reg_alpha': 0.1,\n",
    "                'reg_lambda': 0.1,\n",
    "                'n_jobs': -1,\n",
    "                'random_state': RANDOM_SEED\n",
    "            },\n",
    "            cv_folds,\n",
    "            model_name='lightgbm',\n",
    "            cache_dir=paths['cache_dir']\n",
    "        )\n",
    "        \n",
    "        # Use the better performing model\n",
    "        if ridge_result['mean_score'] < lgb_result['mean_score']:\n",
    "            print(\"Ridge model performed better. Using Ridge for predictions.\")\n",
    "            model_result = ridge_result\n",
    "        else:\n",
    "            print(\"LightGBM model performed better. Using LightGBM for predictions.\")\n",
    "            model_result = lgb_result\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\\\nGenerating predictions...\")\n",
    "    test_preds = generate_predictions(model_result['models'], test_features)\n",
    "    \n",
    "    # Generate submission\n",
    "    submission = generate_submission(test_encoded, test_preds, paths['output_csv_path'])\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model': {\n",
    "            'name': args.model if args.model != 'both' else 'best_of_both',\n",
    "            'mean_smape': model_result['mean_score'],\n",
    "            'fold_smapes': model_result['fold_scores']\n",
    "        },\n",
    "        'runtime_seconds': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(paths['metrics_path'], 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\\\nSaved metrics to {paths['metrics_path']}\")\n",
    "    \n",
    "    # Print final performance\n",
    "    print(\"\\\\n----- Final Performance -----\")\n",
    "    print(f\"Mean SMAPE: {model_result['mean_score']:.4f}\")\n",
    "    print(\"Per-fold SMAPE:\")\n",
    "    for i, score in enumerate(model_result['fold_scores']):\n",
    "        print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "    # Print submission file path\n",
    "    print(f\"\\\\nSubmission file: {paths['output_csv_path']}\")\n",
    "    print(f\"Runtime: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train and predict for Smart Product Pricing Challenge\")\n",
    "    parser.add_argument(\"--base-path\", type=str, default=None, help=\"Base path for input files\")\n",
    "    parser.add_argument(\"--output-path\", type=str, default=None, help=\"Path for output files\")\n",
    "    parser.add_argument(\"--dataset-path\", type=str, default=None, help=\"Path to dataset directory\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"both\", choices=[\"ridge\", \"lightgbm\", \"both\"], \n",
    "                        help=\"Model to train (ridge, lightgbm, or both)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    sys.exit(main(args))\n",
    "'''\n",
    "    \n",
    "    print(f\"Generating train_predict.py script to {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    print(f\"Script generated successfully: {output_path}\")\n",
    "    \n",
    "    # Make script executable\n",
    "    if not output_path.endswith('.py'):\n",
    "        print(\"Warning: Script filename should end with .py\")\n",
    "    else:\n",
    "        try:\n",
    "            os.chmod(output_path, 0o755)  # Make executable\n",
    "            print(\"Script is now executable\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not make script executable: {e}\")\n",
    "    \n",
    "    print(\"\\nTo run the script:\")\n",
    "    print(f\"python {output_path} --output-path /path/to/output\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate README\n",
    "def generate_readme(output_path=None):\n",
    "    \"\"\"Generate README.md with instructions for running the pipeline\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(OUTPUT_PATH, 'README.md')\n",
    "    \n",
    "    readme_content = '''# Smart Product Pricing Challenge\n",
    "\n",
    "This repository contains an end-to-end machine learning pipeline for predicting product prices based on catalog content and image features.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline uses a combination of text processing, feature engineering, and machine learning models to predict product prices. The approach includes:\n",
    "\n",
    "1. Text feature extraction from catalog content\n",
    "2. Image feature extraction using pretrained models (optional)\n",
    "3. Model training with cross-validation\n",
    "4. Ensemble methods for improved performance\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.8+\n",
    "- Required packages: numpy, pandas, scikit-learn, lightgbm, tqdm, joblib\n",
    "- Optional packages: torch, torchvision, sentence_transformers\n",
    "\n",
    "## How to Run\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "The main pipeline is available in `product_price_pipeline.ipynb`. To run it:\n",
    "\n",
    "1. Open the notebook in Jupyter or VS Code\n",
    "2. Run all cells sequentially\n",
    "3. The final submission will be saved as `test_out.csv`\n",
    "\n",
    "### Command-Line Script\n",
    "\n",
    "Alternatively, use the provided command-line script:\n",
    "\n",
    "```bash\n",
    "python train_predict.py --output-path /path/to/output\n",
    "```\n",
    "\n",
    "Options:\n",
    "- `--base-path`: Base path for input files (default: auto-detect)\n",
    "- `--output-path`: Path for output files (default: current directory or /kaggle/working)\n",
    "- `--dataset-path`: Path to dataset directory (default: auto-detect)\n",
    "- `--model`: Model to use - \"ridge\", \"lightgbm\", or \"both\" (default: \"both\")\n",
    "\n",
    "## Quick Baseline\n",
    "\n",
    "For a fast baseline (< 10 minutes runtime), run the \"Quick Baseline\" section in the notebook.\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "The pipeline uses multiple models and ensemble methods to achieve the best performance:\n",
    "\n",
    "1. Ridge Regression on TF-IDF SVD features\n",
    "2. LightGBM on combined features\n",
    "3. Stacking ensemble for final predictions\n",
    "\n",
    "Typical SMAPE performance on cross-validation: ~8-10%\n",
    "\n",
    "## Methodology\n",
    "\n",
    "See `Documentation_template.md` for a detailed explanation of the methodology.\n",
    "'''\n",
    "    \n",
    "    print(f\"Generating README.md to {output_path}\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"README generated successfully: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate files\n",
    "train_predict_path = generate_train_predict_script()\n",
    "readme_path = generate_readme()\n",
    "\n",
    "print(f\"\\nUtility scripts generated:\")\n",
    "print(f\"1. Train/Predict script: {train_predict_path}\")\n",
    "print(f\"2. README file: {readme_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
